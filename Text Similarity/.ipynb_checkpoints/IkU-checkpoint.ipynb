{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Themes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterthemes in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: notebook>=5.6.0 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from jupyterthemes) (5.7.4)\n",
      "Requirement already satisfied: lesscpy>=0.11.2 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from jupyterthemes) (0.13.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from jupyterthemes) (4.4.0)\n",
      "Requirement already satisfied: ipython>=5.4.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from jupyterthemes) (7.2.0)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from jupyterthemes) (3.0.2)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.8.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (17.1.2)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (2.10)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.2.4)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.5.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: tornado>=4 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (1.5.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (4.3.2)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (4.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.12.0)\n",
      "Requirement already satisfied: ply in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (4.3.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.4.1)\n",
      "Requirement already satisfied: pygments in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (2.3.1)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.13.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (2.0.8)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (40.6.2)\n",
      "Requirement already satisfied: numpy>=1.10.0 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.16.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.7.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.0.1)\n",
      "Requirement already satisfied: pywinpty>=0.5; os_name == \"nt\" in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from terminado>=0.8.1->notebook>=5.6.0->jupyterthemes) (0.5.5)\n",
      "Requirement already satisfied: bleach in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (3.1.0)\n",
      "Requirement already satisfied: testpath in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.4.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\n",
      "Requirement already satisfied: mistune>=0.8.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (1.1.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (2.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from jedi>=0.10->ipython>=5.4.1->jupyterthemes) (0.3.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.1.7)\n",
      "Requirement already satisfied: webencodings in c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyterthemes\n",
    "!jt -t monokai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3hUGck-ksJp"
   },
   "source": [
    "# Semantics Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPXZlsook9cd"
   },
   "source": [
    "**Installing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqGfhdAakih2"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q numpy\n",
    "!pip install -U -q keras\n",
    "!pip install -U -q scikit-learn\n",
    "!pip install -U -q matplotlib\n",
    "!pip install -U -q nltk\n",
    "!pip install -U -q PyDrive \n",
    "!pip install -U -q pandas\n",
    "!pip3 install --quiet tensorflow-hub\n",
    "!pip3 install --quiet seaborn\n",
    "!pip3 install --quiet \"tensorflow>=1.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "016VNjCml2NW"
   },
   "source": [
    "**Getting data from Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TEWzFSplqVP"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSqhf7dHl_SS"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqJZtt-HmF6v"
   },
   "outputs": [],
   "source": [
    "file_ids = [\"16-aKOfyeLQpBJlUHCJUGxWp4UsY2rvb3\", \"1oec77bHzg5a2oGshDuBe99jxMi80NlUo\"]\n",
    "file_names = [\"train_translated.csv\", \"test_translated.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLtygnFNmLx_"
   },
   "outputs": [],
   "source": [
    "for each_id, each_name in zip(file_ids, file_names):\n",
    "    download = drive.CreateFile({'id':each_id})\n",
    "    download.GetContentFile(each_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNPMenzA0Yyj"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3Z7qyzq0g6Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Asu7UheH2GU3",
    "outputId": "c7a5f262-2342-40ee-f165-5e1b97bd75c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 102: expected 5 fields, saw 6\\nSkipping line 656: expected 5 fields, saw 6\\nSkipping line 867: expected 5 fields, saw 6\\nSkipping line 880: expected 5 fields, saw 6\\nSkipping line 980: expected 5 fields, saw 6\\nSkipping line 1439: expected 5 fields, saw 6\\nSkipping line 1473: expected 5 fields, saw 6\\nSkipping line 1822: expected 5 fields, saw 6\\nSkipping line 1952: expected 5 fields, saw 6\\nSkipping line 2009: expected 5 fields, saw 6\\nSkipping line 2230: expected 5 fields, saw 6\\nSkipping line 2506: expected 5 fields, saw 6\\nSkipping line 2523: expected 5 fields, saw 6\\nSkipping line 2809: expected 5 fields, saw 6\\nSkipping line 2887: expected 5 fields, saw 6\\nSkipping line 2920: expected 5 fields, saw 6\\nSkipping line 2944: expected 5 fields, saw 6\\nSkipping line 3241: expected 5 fields, saw 6\\nSkipping line 3358: expected 5 fields, saw 6\\nSkipping line 3459: expected 5 fields, saw 6\\n'\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"datasets/msr-para-train.tsv\", sep='\\t', error_bad_lines=False, skip_blank_lines=True, keep_default_na=False)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_Anv5AYHe5X"
   },
   "outputs": [],
   "source": [
    "test1 = test.iloc[:, 3].values\n",
    "test2 = test.iloc[:, 4].values\n",
    "res = test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\"datasets/questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions1 = questions.iloc[:, 3].values\n",
    "questions2 = questions.iloc[:, 4].values\n",
    "is_duplicate_questions = questions.iloc[:, 5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjk1awi9vns5"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNVsrA9WHe5b"
   },
   "outputs": [],
   "source": [
    "length = res.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7q4ZLpcKj0v"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VsMTUtGdKoJM"
   },
   "source": [
    "**Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TEybJIOKkgT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "uRnXzsycKsMg",
    "outputId": "8b41cc96-5296-44c5-ca5a-9d62266da722",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vsriv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vsriv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTPkxZdiK0BD"
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "stopword = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd75AVttK7hD"
   },
   "outputs": [],
   "source": [
    "dataset_p_l_rms_l_1 = []\n",
    "dataset_p_l_rms_1 = []\n",
    "for i in questions1:\n",
    "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
    "    tempx = tempx.lower().split()\n",
    "    tempx = [word for word in tempx if word not in stopword]\n",
    "    dataset_p_l_rms_1.append(\" \".join(tempx))\n",
    "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
    "    dataset_p_l_rms_l_1.append(\" \".join(tempx))\n",
    "dataset_p_l_rms_l_1 = np.asarray(dataset_p_l_rms_l1)\n",
    "dataset_p_l_rms_1 = np.asarray(dataset_p_l_rms_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdAWSHcJHe5q"
   },
   "outputs": [],
   "source": [
    "dataset_p_l_rms_l_2 = []\n",
    "dataset_p_l_rms_2 = []\n",
    "for i in questions2:\n",
    "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
    "    tempx = tempx.lower().split()\n",
    "    tempx = [word for word in tempx if word not in stopword]\n",
    "    dataset_p_l_rms_2.append(\" \".join(tempx))\n",
    "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
    "    dataset_p_l_rms_l_2.append(\" \".join(tempx))\n",
    "dataset_p_l_rms_l_2 = np.asarray(dataset_p_l_rms_l_2)\n",
    "dataset_p_l_rms_2 = np.asarray(dataset_p_l_rms_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "max_words = 0\n",
    "for each in dataset_p_l_rms_l_1:\n",
    "    max_words = max(len(each.split()), max_words)\n",
    "\n",
    "for each in dataset_p_l_rms_l_2:\n",
    "    max_words = max(len(each.split()), max_words)\n",
    "print(max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorizor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.fit(np.append(train, test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train = count_vectorizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test = count_vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tfidf Vectorizor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.fit(np.append(train, test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train = tfidf_vectorizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test = tfidf_vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-xcGFtPGdfJ"
   },
   "source": [
    "**LSA Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYXzCvrQGgm3"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHMV-1Bhai1e"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DObvs6mal97"
   },
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=300,\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__L-nT-lapFP"
   },
   "outputs": [],
   "source": [
    "lsa_model1 = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCFrhTD-He55"
   },
   "outputs": [],
   "source": [
    "lsa_model2 = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0Qm7cU-asMM"
   },
   "outputs": [],
   "source": [
    "lsa_test1 = lsa_model1.fit_transform(dataset_p_l_rms_l_1)\n",
    "lsa_test2 = lsa_model2.fit_transform(dataset_p_l_rms_l_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShsDeBj0He59"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-RqCVPvHe5_"
   },
   "outputs": [],
   "source": [
    "from math import isnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHVZTTRYHe6A",
    "outputId": "cc91ef3d-720c-4269-f4f8-c6729caa3889"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "founded_lsa = []\n",
    "for i in range(lsa_test1.shape[0]):\n",
    "    temp = cosine(lsa_test1[i], lsa_test2[i])\n",
    "    if not isnan(temp):\n",
    "        founded_lsa.append(1 - int(temp))\n",
    "    else:\n",
    "        founded_lsa.append(0)\n",
    "founded_lsa = np.asarray(founded_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uENvSaywLgrP"
   },
   "source": [
    "**Word2Vec model(Using Mean to get the sentence vectors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_vSYQP8LN36"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61vPjXTNB8Ur"
   },
   "outputs": [],
   "source": [
    "wiki_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/wiki/wiki.300d.txt\", binary=False)\n",
    "#google_model = KeyedVectors.load_word2vec_format(\"models/pretrained/word2vec/google/google.300d.bin\", binary=True)\n",
    "#common_crawl_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/common_crawl/common_crawl.300d.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOUcqYwzLzTf"
   },
   "outputs": [],
   "source": [
    "def sentence_vectorizer(model, sentence):\n",
    "    vectors =[]\n",
    "    num = 0\n",
    "    for i in sentence.split():\n",
    "        try:\n",
    "            if num == 0:\n",
    "                vectors = model[i]\n",
    "            else:\n",
    "                vectors = np.add(vectors, model[i])\n",
    "            num += 1\n",
    "        except:\n",
    "            pass\n",
    "    return np.array(vectors) / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsR-G4WYsu70"
   },
   "outputs": [],
   "source": [
    "sent_vec1 = []\n",
    "for each in dataset_p_l_rms_1:\n",
    "    temp = sentence_vectorizer(google_model, each)\n",
    "    if temp.shape[0] != 0:\n",
    "        sent_vec1.append(temp)\n",
    "    else:\n",
    "        sent_vec1.append(np.zeros((300,)))\n",
    "sent_vec1 = np.asarray(sent_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EsdRpmHGhpM"
   },
   "outputs": [],
   "source": [
    "sent_vec2 = []\n",
    "for each in dataset_p_l_rms_2:\n",
    "    temp = sentence_vectorizer(google_model, each)\n",
    "    if temp.shape[0] != 0:\n",
    "        sent_vec2.append(temp)\n",
    "    else:\n",
    "        sent_vec2.append(np.zeros((300,)))\n",
    "sent_vec2 = np.asarray(sent_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STeo9-I-He6R"
   },
   "outputs": [],
   "source": [
    "founded_sent2vec = []\n",
    "for i in range(length):\n",
    "    temp = cosine(sent_vec1[i], sent_vec2[i])\n",
    "    if not isnan(temp):\n",
    "        founded_sent2vec.append(1 - int(temp))\n",
    "    else:\n",
    "        founded_sent2vec.append(0)\n",
    "founded_sent2vec = np.asarray(founded_sent2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sent2Vec model(Fast.ai)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/epfml/sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Encoder V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join(\n",
    "            (str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "        corr,\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):\n",
    "    message_embeddings_ = session_.run(\n",
    "        encoding_tensor, feed_dict={input_tensor_: messages_})\n",
    "    plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"I am an Indian\",\n",
    "    \"I am from India\",\n",
    "    \"I am not from India\",\n",
    "    \"I play cricket\",\n",
    "    \"I watch television\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    run_and_plot(session, similarity_input_placeholder, messages,\n",
    "         similarity_message_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elQTFbMuHhgN"
   },
   "source": [
    "**Siamese Neural Networks(Using LSTM and GRU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2639,
     "status": "ok",
     "timestamp": 1549095603688,
     "user": {
      "displayName": "Sri Vatsan",
      "photoUrl": "",
      "userId": "12441326585716590406"
     },
     "user_tz": -330
    },
    "id": "dE_9Xb7uHqgw",
    "outputId": "84a6f14b-0c50-4bb8-d8f5-b431d981f11b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as backend\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda, GRU, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(test1) + 1, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 512\n",
    "n_hidden2 = 384\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WljRtYIvJcTn"
   },
   "outputs": [],
   "source": [
    "left_input = Input(shape=(max_words, ), dtype='int32')\n",
    "right_input = Input(shape=(max_words, ), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksWe8MukKbxW"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], \n",
    "                            input_length=max_words, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0jTBGv4Kck8"
   },
   "outputs": [],
   "source": [
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5e9B1ChNKgA3"
   },
   "outputs": [],
   "source": [
    "shared_lstm1 = LSTM(n_hidden1, return_sequences=True)\n",
    "shared_dropout1 = Dropout(0.3)\n",
    "shared_gru1 = GRU(n_hidden2, return_sequences=True)\n",
    "shared_dropout2 = Dropout(0.4)\n",
    "shared_gru2 = GRU(n_hidden3, return_sequences=True)\n",
    "shared_dropout3 = Dropout(0.3)\n",
    "shared_lstm2 = LSTM(n_hidden4, return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiSpiz9uLGu3"
   },
   "outputs": [],
   "source": [
    "left_lstm1 = shared_lstm1(encoded_left)\n",
    "left_dropout1 = shared_dropout1(left_lstm1)\n",
    "left_gru1 = shared_gru1(left_dropout1)\n",
    "left_dropout2 = shared_dropout2(left_gru1)\n",
    "left_gru2 = shared_gru2(left_dropout2)\n",
    "left_dropout3 = shared_dropout3(left_gru2)\n",
    "left_lstm2 = shared_lstm2(left_dropout3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcQJfYn6Ma5S"
   },
   "outputs": [],
   "source": [
    "right_lstm1 = shared_lstm1(encoded_right)\n",
    "right_dropout1 = shared_dropout1(right_lstm1)\n",
    "right_gru1 = shared_gru1(right_dropout1)\n",
    "right_dropout2 = shared_dropout2(right_gru1)\n",
    "right_gru2 = shared_gru2(right_dropout2)\n",
    "right_dropout3 = shared_dropout3(right_gru2)\n",
    "right_lstm2 = shared_lstm2(right_dropout3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ug1R5gTGM4oR"
   },
   "outputs": [],
   "source": [
    "manhattan_distance_for_lstm = Lambda(function=lambda x: backend.exp(-backend.sum(backend.abs(x[0]-x[1]), axis=1, keepdims=True)),\n",
    "                                     output_shape=lambda x: (x[0][0], 1))([left_lstm2, right_lstm2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dWiDq3dUK7m"
   },
   "source": [
    "**Training and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fkjFhAkT8Ov"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNlq-mXyUjO9"
   },
   "outputs": [],
   "source": [
    "stratkfold = StratifiedKFold(n_splits=2, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5TGTK2CUrN8"
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in stratkfold.split(test1, res):\n",
    "    siamese_network = Model([left_input, right_input], manhattan_distance_for_lstm)\n",
    "    siamese_network.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    siamese_network.fit([test1[train_index], test2[train_index]], res[train_index], batch_size=128, \n",
    "                        epochs=128, validation_data=([test1[test_index], test2[test_index]], res[test_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Erk3iRTcW9Lb"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SqSI60RXDd2"
   },
   "outputs": [],
   "source": [
    "siamese_network = Model([left_input, right_input], manhattan_distance_for_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gumorqrYXDOS"
   },
   "outputs": [],
   "source": [
    "siamese_network.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WSGBgGwXC_R"
   },
   "outputs": [],
   "source": [
    "siamese_network.fit([test1[train_index], test2[train_index]], res[train_index], batch_size=128, epochs=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGyxnRuCHe6T"
   },
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8U9QUzGUHe6U"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Em0idSRBHe6V"
   },
   "source": [
    "**LSA Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5x_Xej09He6W",
    "outputId": "fc2d20a4-3cae-4cc5-e2e3-df4a68825d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.4665402088779303\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score :\", accuracy_score(is_duplicate_questions, founded_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyzrfpRWHe6Y",
    "outputId": "36ee63ac-6c07-491c-8cab-02411ba57d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.39018605805011164\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision :\", precision_score(is_duplicate_questions, founded_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_7dXh_gHe6a",
    "outputId": "34f22ea6-6364-4458-b44b-f03d32ebe9ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score : 0.7900754155894606\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall Score :\", recall_score(is_duplicate_questions, founded_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAcIrkueHe6d",
    "outputId": "b7109c07-77ea-4111-c841-cc0c5276cdf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.5223866386496941\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score :\", f1_score(is_duplicate_questions, founded_lsa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6qjSGvAHe6f"
   },
   "source": [
    "**Sentence2Vec(Modified word2vec) Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8263tpeHe6h",
    "outputId": "bacfcf86-1d83-45a6-98db-ed325c5e5bb5"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy Score :\", accuracy_score(res, founded_sent2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lWthDUYRHe6k",
    "outputId": "8ffab08e-7e47-49fb-8eaf-6c1566061948"
   },
   "outputs": [],
   "source": [
    "print(\"Precision :\", precision_score(res, founded_sent2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEKGf4lgHe6m",
    "outputId": "ef8472e3-d3dc-43ce-a480-e108f601bc22"
   },
   "outputs": [],
   "source": [
    "print(\"Recall :\", recall_score(res, founded_sent2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRLQnmzoHe6r",
    "outputId": "1d54178c-89ed-4fd0-aa31-eb01dd8fe3c3"
   },
   "outputs": [],
   "source": [
    "print(\"F1 Score :\", f1_score(res, founded_sent2vec))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Semantics_Similarity.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
