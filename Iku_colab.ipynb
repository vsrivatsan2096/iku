{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iku_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsrivatsan2096/iku/blob/master/Iku_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "v03y6d1u3ESI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installing Libraries"
      ]
    },
    {
      "metadata": {
        "id": "EXIZLLru3DmH",
        "colab_type": "code",
        "outputId": "9cb70d6a-71f9-41c7-b60f-0bb3504d9e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q numpy\n",
        "!pip install -U -q keras\n",
        "!pip install -U -q scikit-learn\n",
        "!pip install -U -q matplotlib\n",
        "!pip install -U -q nltk\n",
        "!pip install -U -q PyDrive \n",
        "!pip install -U -q pandas\n",
        "!pip install https://download.pytorch.org/whl/cu100/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install -U -q torchvision\n",
        "!pip install --quiet tensorflow-hub\n",
        "!pip install --quiet seaborn\n",
        "!pip install --quiet \"tensorflow>=1.7\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 17.3MB 1.7MB/s \n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 12.5MB/s \n",
            "\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 21.0MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[K    100% |████████████████████████████████| 10.1MB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.0.1.post2 from https://download.pytorch.org/whl/cu100/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U3hUGck-ksJp"
      },
      "cell_type": "markdown",
      "source": [
        "# Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "h4cispv0xwXq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_mD6aJ7Ex4ZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_0PElT316uN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_ids = [\"1JslQ15FgNAcEdCsKGAGkOjOBBTq_jaUw\", \"1q7pTs8JeD7o5v2P38C0FrbXJGErPFfWZ\"]\n",
        "file_names = ['google-embeddings-w2v.gz', 'common-crawl-embeddings-w2v.txt']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BfixdvYi3Sf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "metadata": {
        "id": "62kaJjFn3RAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihpZN05s3Zvp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Cloning Github"
      ]
    },
    {
      "metadata": {
        "id": "51aCgGvI3aRC",
        "colab_type": "code",
        "outputId": "f63dcbdd-c1aa-4f31-f064-988dd6e8dd68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vsrivatsan2096/iku.git"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'iku'...\n",
            "remote: Enumerating objects: 226, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/226)   \u001b[K\rremote: Counting objects:   1% (3/226)   \u001b[K\rremote: Counting objects:   2% (5/226)   \u001b[K\rremote: Counting objects:   3% (7/226)   \u001b[K\rremote: Counting objects:   4% (10/226)   \u001b[K\rremote: Counting objects:   5% (12/226)   \u001b[K\rremote: Counting objects:   6% (14/226)   \u001b[K\rremote: Counting objects:   7% (16/226)   \u001b[K\rremote: Counting objects:   8% (19/226)   \u001b[K\rremote: Counting objects:   9% (21/226)   \u001b[K\rremote: Counting objects:  10% (23/226)   \u001b[K\rremote: Counting objects:  11% (25/226)   \u001b[K\rremote: Counting objects:  12% (28/226)   \u001b[K\rremote: Counting objects:  13% (30/226)   \u001b[K\rremote: Counting objects:  14% (32/226)   \u001b[K\rremote: Counting objects:  15% (34/226)   \u001b[K\rremote: Counting objects:  16% (37/226)   \u001b[K\rremote: Counting objects:  17% (39/226)   \u001b[K\rremote: Counting objects:  18% (41/226)   \u001b[K\rremote: Counting objects:  19% (43/226)   \u001b[K\rremote: Counting objects:  20% (46/226)   \u001b[K\rremote: Counting objects:  21% (48/226)   \u001b[K\rremote: Counting objects:  22% (50/226)   \u001b[K\rremote: Counting objects:  23% (52/226)   \u001b[K\rremote: Counting objects:  24% (55/226)   \u001b[K\rremote: Counting objects:  25% (57/226)   \u001b[K\rremote: Counting objects:  26% (59/226)   \u001b[K\rremote: Counting objects:  27% (62/226)   \u001b[K\rremote: Counting objects:  28% (64/226)   \u001b[K\rremote: Counting objects:  29% (66/226)   \u001b[K\rremote: Counting objects:  30% (68/226)   \u001b[K\rremote: Counting objects:  31% (71/226)   \u001b[K\rremote: Counting objects:  32% (73/226)   \u001b[K\rremote: Counting objects:  33% (75/226)   \u001b[K\rremote: Counting objects:  34% (77/226)   \u001b[K\rremote: Counting objects:  35% (80/226)   \u001b[K\rremote: Counting objects:  36% (82/226)   \u001b[K\rremote: Counting objects:  37% (84/226)   \u001b[K\rremote: Counting objects:  38% (86/226)   \u001b[K\rremote: Counting objects:  39% (89/226)   \u001b[K\rremote: Counting objects:  40% (91/226)   \u001b[K\rremote: Counting objects:  41% (93/226)   \u001b[K\rremote: Counting objects:  42% (95/226)   \u001b[K\rremote: Counting objects:  43% (98/226)   \u001b[K\rremote: Counting objects:  44% (100/226)   \u001b[K\rremote: Counting objects:  45% (102/226)   \u001b[K\rremote: Counting objects:  46% (104/226)   \u001b[K\rremote: Counting objects:  47% (107/226)   \u001b[K\rremote: Counting objects:  48% (109/226)   \u001b[K\rremote: Counting objects:  49% (111/226)   \u001b[K\rremote: Counting objects:  50% (113/226)   \u001b[K\rremote: Counting objects:  51% (116/226)   \u001b[K\rremote: Counting objects:  52% (118/226)   \u001b[K\rremote: Counting objects:  53% (120/226)   \u001b[K\rremote: Counting objects:  54% (123/226)   \u001b[K\rremote: Counting objects:  55% (125/226)   \u001b[K\rremote: Counting objects:  56% (127/226)   \u001b[K\rremote: Counting objects:  57% (129/226)   \u001b[K\rremote: Counting objects:  58% (132/226)   \u001b[K\rremote: Counting objects:  59% (134/226)   \u001b[K\rremote: Counting objects:  60% (136/226)   \u001b[K\rremote: Counting objects:  61% (138/226)   \u001b[K\rremote: Counting objects:  62% (141/226)   \u001b[K\rremote: Counting objects:  63% (143/226)   \u001b[K\rremote: Counting objects:  64% (145/226)   \u001b[K\rremote: Counting objects:  65% (147/226)   \u001b[K\rremote: Counting objects:  66% (150/226)   \u001b[K\rremote: Counting objects:  67% (152/226)   \u001b[K\rremote: Counting objects:  68% (154/226)   \u001b[K\rremote: Counting objects:  69% (156/226)   \u001b[K\rremote: Counting objects:  70% (159/226)   \u001b[K\rremote: Counting objects:  71% (161/226)   \u001b[K\rremote: Counting objects:  72% (163/226)   \u001b[K\rremote: Counting objects:  73% (165/226)   \u001b[K\rremote: Counting objects:  74% (168/226)   \u001b[K\rremote: Counting objects:  75% (170/226)   \u001b[K\rremote: Counting objects:  76% (172/226)   \u001b[K\rremote: Counting objects:  77% (175/226)   \u001b[K\rremote: Counting objects:  78% (177/226)   \u001b[K\rremote: Counting objects:  79% (179/226)   \u001b[K\rremote: Counting objects:  80% (181/226)   \u001b[K\rremote: Counting objects:  81% (184/226)   \u001b[K\rremote: Counting objects:  82% (186/226)   \u001b[K\rremote: Counting objects:  83% (188/226)   \u001b[K\rremote: Counting objects:  84% (190/226)   \u001b[K\rremote: Counting objects:  85% (193/226)   \u001b[K\rremote: Counting objects:  86% (195/226)   \u001b[K\rremote: Counting objects:  87% (197/226)   \u001b[K\rremote: Counting objects:  88% (199/226)   \u001b[K\rremote: Counting objects:  89% (202/226)   \u001b[K\rremote: Counting objects:  90% (204/226)   \u001b[K\rremote: Counting objects:  91% (206/226)   \u001b[K\rremote: Counting objects:  92% (208/226)   \u001b[K\rremote: Counting objects:  93% (211/226)   \u001b[K\rremote: Counting objects:  94% (213/226)   \u001b[K\rremote: Counting objects:  95% (215/226)   \u001b[K\rremote: Counting objects:  96% (217/226)   \u001b[K\rremote: Counting objects:  97% (220/226)   \u001b[K\rremote: Counting objects:  98% (222/226)   \u001b[K\rremote: Counting objects:  99% (224/226)   \u001b[K\rremote: Counting objects: 100% (226/226)   \u001b[K\rremote: Counting objects: 100% (226/226), done.\u001b[K\n",
            "remote: Compressing objects: 100% (179/179), done.\u001b[K\n",
            "remote: Total 226 (delta 64), reused 174 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (226/226), 10.16 MiB | 16.60 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UUUWhTPN2pEx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-Trained embeddings"
      ]
    },
    {
      "metadata": {
        "id": "W_bFeAAX2AmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Google\n",
        "download = drive.CreateFile({'id':file_ids[0]})\n",
        "download.GetContentFile(file_names[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKSkSTVlQwuh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "google_embeddings = 'google-embeddings-w2v.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l5572PWOWV4A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Common Crawl\n",
        "download = drive.CreateFile({'id':file_ids[1]})\n",
        "download.GetContentFile(file_names[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uS0JKzfWQyVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "common_crawl_embeddings = 'common-crawl-embeddings-w2v.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPGycXAI7Shk",
        "colab_type": "code",
        "outputId": "3be188c8-c98d-475c-b5c8-245f9f05eaf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/infersent/infersent1.pkl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-17 05:56:36--  https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154010676 (147M) [application/octet-stream]\n",
            "Saving to: ‘infersent1.pkl’\n",
            "\n",
            "infersent1.pkl      100%[===================>] 146.88M  43.5MB/s    in 4.9s    \n",
            "\n",
            "2019-03-17 05:56:42 (30.3 MB/s) - ‘infersent1.pkl’ saved [154010676/154010676]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDzrm_5HCEN0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "infersent_embeddings = 'infersent1.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gLWw8RoGB9WP",
        "colab_type": "code",
        "outputId": "555e2b5a-2891-4222-84ae-ad2a6df96bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-17 12:48:54--  https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154010676 (147M) [application/octet-stream]\n",
            "Saving to: ‘infersent2.pkl’\n",
            "\n",
            "infersent2.pkl      100%[===================>] 146.88M   133MB/s    in 1.1s    \n",
            "\n",
            "2019-03-17 12:48:55 (133 MB/s) - ‘infersent2.pkl’ saved [154010676/154010676]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bA6HIqiwR70P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "infersent_embeddings = 'infersent2.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SHHp9JSkurNo",
        "colab_type": "code",
        "outputId": "cb9a07f6-7bb7-4ff4-ce3f-906c1d335d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-17 12:49:13--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5828358084 (5.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M-subword.zip’\n",
            "\n",
            "crawl-300d-2M-subwo 100%[===================>]   5.43G  37.2MB/s    in 3m 51s  \n",
            "\n",
            "2019-03-17 12:53:05 (24.1 MB/s) - ‘crawl-300d-2M-subword.zip’ saved [5828358084/5828358084]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZwRq38MZuuOb",
        "colab_type": "code",
        "outputId": "b8386d43-2e2d-441c-d995-36121bbbcc85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip crawl-300d-2M-subword.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  crawl-300d-2M-subword.zip\n",
            "  inflating: crawl-300d-2M-subword.vec  \n",
            "  inflating: crawl-300d-2M-subword.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5lefexLrzSSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "infersent_crawl_embeddings = \"crawl-300d-2M-subword.vec\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HNPMenzA0Yyj"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "id": "n9M8WO8l29is",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_ids = [\"1bqdQccb6176JhXU7CZbWnv3vTAi7BH_-\"]\n",
        "file_names = ['questions.csv']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OKUfynKS29a6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for each_id, each_name in zip(file_ids, file_names):\n",
        "    download = drive.CreateFile({'id':each_id})\n",
        "    download.GetContentFile(each_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WIRMSQ2aYFOW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del download\n",
        "del gauth\n",
        "del drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "J3Z7qyzq0g6Q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import multiprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bM1togrSTgrm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions = pd.read_csv(\"questions.csv\")\n",
        "questions.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N3hpXv-GTgup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions1 = questions.iloc[:, 3].values\n",
        "questions2 = questions.iloc[:, 4].values\n",
        "is_duplicate_questions = questions.iloc[:, 5].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVTLX122TgyM",
        "colab_type": "code",
        "outputId": "6052319a-476d-46ba-df35-0a7cd8587654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "questions.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  qid2                                          question1  \\\n",
              "0   0     1     2  What is the step by step guide to invest in sh...   \n",
              "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
              "2   2     5     6  How can I increase the speed of my internet co...   \n",
              "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
              "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
              "\n",
              "                                           question2  is_duplicate  \n",
              "0  What is the step by step guide to invest in sh...             0  \n",
              "1  What would happen if the Indian government sto...             0  \n",
              "2  How can Internet speed be increased by hacking...             0  \n",
              "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
              "4            Which fish would survive in salt water?             0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sjk1awi9vns5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cores = multiprocessing.cpu_count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mNVsrA9WHe5b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "length = is_duplicate_questions.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1euoTy-7qr2w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_scores = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCi7z0cr8tIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scores"
      ]
    },
    {
      "metadata": {
        "id": "AfDwdfMJ1sVZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/makcedward/nlp/blob/master/sample/nlp-word_mover_distance.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oF9or-MN8xDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "from sklearn.neighbors import DistanceMetric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E7OiGlv2-Ikq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cosine_similarity_scores(sentence1, sentence2):\n",
        "    return cosine_similarity(sentence1.reshape(1, -1), sentence2.reshape(1, -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OI-2pT5A-IcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def manhattan_distances_scores(sentence1, sentence2):\n",
        "    return manhattan_distances(sentence1.reshape(1, -1), sentence2.reshape(1, -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U5Qz5hkM-IVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def euclidean_distances_scores(sentence1, sentence2):\n",
        "    return euclidean_distances(sentence1.reshape(1, -1), sentence2.reshape(1, -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4iQZeg5V-IPh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def jaccard_similarity_scores(sentence1, sentence2):\n",
        "    jac_score = 0\n",
        "    try:\n",
        "        jac_score = jaccard_similarity_score(sentence1.reshape(1, -1), sentence2.reshape(1, -1))\n",
        "    finally:\n",
        "        return jac_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_58YfrGX-N-8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def minkowski_distances_scores(sentence1, sentence2):\n",
        "    minkowski_distance = DistanceMetric.get_metric('minkowski')\n",
        "    return minkowski_distance.pairwise(sentence1.reshape(1, -1), sentence2.reshape(1, -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "slLOTw3x-Sb8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_scores(sentences1, sentences2, length):\n",
        "    scores = []\n",
        "    for each in range(length):\n",
        "        each_score = []\n",
        "        each_score.append(cosine_similarity_scores(sentences1[each], sentences2[each])[0][0])\n",
        "        each_score.append(manhattan_distances_scores(sentences1[each], sentences2[each])[0][0])\n",
        "        each_score.append(euclidean_distances_scores(sentences1[each], sentences2[each])[0][0])\n",
        "        #each_score.append(self.jaccard_similarity(self.sentences1[each], self.sentences2[each]))\n",
        "        each_score.append(minkowski_distances_scores(sentences1[each], sentences2[each])[0][0])\n",
        "        scores.append(np.asarray(each_score))\n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "k7q4ZLpcKj0v"
      },
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3TEybJIOKkgT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uRnXzsycKsMg",
        "outputId": "f978a0e1-54a6-43b7-d96f-7e78d390ce8f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KTPkxZdiK0BD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "stopword = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rd75AVttK7hD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences_1 = []\n",
        "for i in questions1:\n",
        "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
        "    tempx = tempx.lower().split()\n",
        "    tempx = [word for word in tempx if word not in stopword]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
        "    sentences_1.append(\" \".join(tempx))\n",
        "sentences_1 = np.asarray(sentences_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HdAWSHcJHe5q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences_2 = []\n",
        "for i in questions2:\n",
        "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
        "    tempx = tempx.lower().split()\n",
        "    tempx = [word for word in tempx if word not in stopword]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
        "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
        "    sentences_2.append(\" \".join(tempx))\n",
        "sentences_2 = np.asarray(sentences_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "w-xcGFtPGdfJ"
      },
      "cell_type": "markdown",
      "source": [
        "# LSA Method"
      ]
    },
    {
      "metadata": {
        "id": "bT1FLns41CbI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jYXzCvrQGgm3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LHMV-1Bhai1e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2DObvs6mal97",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svd_model = TruncatedSVD(n_components=300,\n",
        "                         algorithm='randomized',\n",
        "                         n_iter=10, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "__L-nT-lapFP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lsa_model1 = Pipeline([('tfidf', vectorizer), \n",
        "                            ('svd', svd_model)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cCFrhTD-He55",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lsa_model2 = Pipeline([('tfidf', vectorizer), \n",
        "                            ('svd', svd_model)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "d0Qm7cU-asMM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lsa_1 = lsa_model1.fit_transform(sentences_1)\n",
        "lsa_2 = lsa_model2.fit_transform(sentences_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ShsDeBj0He59",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_scores['lsa'] = get_scores(lsa_1, lsa_2, length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6iSsDaItQZy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del vectorizer\n",
        "del svd_model\n",
        "del lsa_model1\n",
        "del lsa_model2\n",
        "del lsa_1\n",
        "del lsa_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uENvSaywLgrP"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec model(Using Mean to get the sentence vectors)"
      ]
    },
    {
      "metadata": {
        "id": "a2F2zLN101j1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "n_vSYQP8LN36",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "61vPjXTNB8Ur",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "google_model = KeyedVectors.load_word2vec_format(google_embeddings, binary=True)\n",
        "#wiki_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/wiki/wiki.300d.txt\", binary=False)\n",
        "#common_crawl_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/common_crawl/common_crawl.300d.txt\", binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tOUcqYwzLzTf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentence_vectorizer(model, sentence):\n",
        "    vectors =[]\n",
        "    num = 0\n",
        "    for i in sentence.split():\n",
        "        try:\n",
        "            if num == 0:\n",
        "                vectors = model[i]\n",
        "            else:\n",
        "                vectors = np.add(vectors, model[i])\n",
        "            num += 1\n",
        "        except:\n",
        "            pass\n",
        "    return np.array(vectors) / num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xsR-G4WYsu70",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent_vec1 = []\n",
        "for each in sentences_1:\n",
        "    temp = sentence_vectorizer(google_model, each)\n",
        "    if temp.shape[0] != 0:\n",
        "        sent_vec1.append(temp)\n",
        "    else:\n",
        "        sent_vec1.append(np.zeros((300,)))\n",
        "sent_vec1 = np.asarray(sent_vec1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4EsdRpmHGhpM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sent_vec2 = []\n",
        "for each in sentences_2:\n",
        "    temp = sentence_vectorizer(google_model, each)\n",
        "    if temp.shape[0] != 0:\n",
        "        sent_vec2.append(temp)\n",
        "    else:\n",
        "        sent_vec2.append(np.zeros((300,)))\n",
        "sent_vec2 = np.asarray(sent_vec2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kw-hOFYytmBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_scores['sentence vector'] = get_scores(sent_vec1, sent_vec2, length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Udv8gt2Ut8pd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del google_model\n",
        "del sent_vec1\n",
        "del sent_vec2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NPFU-WBzThkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# InferText model"
      ]
    },
    {
      "metadata": {
        "id": "GjDUVOyGThkU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/facebookresearch/InferSent"
      ]
    },
    {
      "metadata": {
        "id": "6dv_PWSvThkV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from random import randint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "josZK7nlThkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Load Model**"
      ]
    },
    {
      "metadata": {
        "id": "n6zpFS63Thk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from iku.methods.infersent.models import InferSent\n",
        "model_version = 2\n",
        "params_model = {'bsize': 128, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
        "model = InferSent(params_model)\n",
        "model.load_state_dict(torch.load(infersent_embeddings))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3Ab2lSGThlU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_cuda = False\n",
        "model = model.cuda() if use_cuda else model # Keep it on CPU or put it on GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGK1B7e0ThlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W2V_PATH = infersent_crawl_embeddings\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Wpz7wfJThld",
        "colab_type": "code",
        "outputId": "29928ba9-b6f7-4193-e428-52f25d4aa7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.build_vocab_k_words(K=100000)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1llZQxTQThme",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Encode Sentences**"
      ]
    },
    {
      "metadata": {
        "id": "06NrXp-Bx2TP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = 50000 # All the data crashes the RAM and GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uUSy7wA4Yru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_scores['sentence vector'] = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tW3EZmx8Thm3",
        "colab_type": "code",
        "outputId": "86be4b4a-5e0d-410b-92d3-08a11ba6ab3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, length, batch):\n",
        "    infersent_1 = model.encode(sentences_1[i:i+batch], bsize=128, tokenize=True, verbose=True)\n",
        "    infersent_2 = model.encode(sentences_2[i:i+batch], bsize=128, tokenize=True, verbose=True)\n",
        "    X_scores['sentence vector'].extend(get_scores(infersent_1, infersent_2, infersent_1.shape[0]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nb words kept : 351813/368855 (95.4%)\n",
            "Speed : 2100.9 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 356434/372214 (95.8%)\n",
            "Speed : 2059.0 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 351782/368682 (95.4%)\n",
            "Speed : 2129.8 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 356071/371723 (95.8%)\n",
            "Speed : 2174.5 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 351675/368630 (95.4%)\n",
            "Speed : 2203.2 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 356694/372433 (95.8%)\n",
            "Speed : 2183.4 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 352474/369541 (95.4%)\n",
            "Speed : 2070.5 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 357360/373032 (95.8%)\n",
            "Speed : 2113.5 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 352527/369219 (95.5%)\n",
            "Speed : 2152.7 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 355519/371279 (95.8%)\n",
            "Speed : 2176.7 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 352511/369636 (95.4%)\n",
            "Speed : 2191.9 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 357175/372840 (95.8%)\n",
            "Speed : 2176.2 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 352007/369391 (95.3%)\n",
            "Speed : 2193.4 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 355872/371765 (95.7%)\n",
            "Speed : 2050.0 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 353806/370992 (95.4%)\n",
            "Speed : 2110.7 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 357921/373821 (95.7%)\n",
            "Speed : 2123.9 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 30805/32253 (95.5%)\n",
            "Speed : 1938.3 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 31302/32684 (95.8%)\n",
            "Speed : 1992.6 sentences/s (gpu mode, bsize=128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BOdJ4K5Rt6RT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del model\n",
        "del infersent_1\n",
        "del infersent_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lIkMtICSThoZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentence Encoder V2"
      ]
    },
    {
      "metadata": {
        "id": "aL2-FeJ-Thoa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://tfhub.dev/google/universal-sentence-encoder/2"
      ]
    },
    {
      "metadata": {
        "id": "eCCuUdvz7ywM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JXUgrAajThpr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kbKz3ohbAyNl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed = hub.Module(module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yV3hBbht_vsQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_scores['sentence encoder'] = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3qyXxv6fvqo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = 50000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L38yTVryr99G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run in cpu since GPU wont help\n",
        "config = tf.ConfigProto(\n",
        "        device_count = {'GPU': 0}\n",
        "    )\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-jIYTgxThqq",
        "colab_type": "code",
        "outputId": "354b0e93-a81a-41d7-be85-084548abb73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(config=config) as session:\n",
        "    if tf.test.gpu_device_name():\n",
        "        print('GPU found')\n",
        "    else:\n",
        "        print(\"No GPU found\")\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    for i in range(0, length, batch):\n",
        "        print(i)\n",
        "        sentences_embeddings_1 = session.run(embed(sentences_1[i:i+batch].tolist()))\n",
        "        sentences_embeddings_2 = session.run(embed(sentences_2[i:i+batch].tolist()))\n",
        "        X_scores['sentence encoder'].extend(get_scores(sentences_embeddings_1, sentences_embeddings_2, sentences_embeddings_1.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU found\n",
            "0\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0317 17:27:13.345176 140347005482880 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0317 17:27:49.978174 140347005482880 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0317 17:29:03.388684 140347005482880 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pK6kZBTGujgr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del embed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0REIjwVNThsP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN and TimeDistributed"
      ]
    },
    {
      "metadata": {
        "id": "ZTLQ5ZJNThsQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/zhihang/an-ensemble-approach-cnn-and-timedistributed"
      ]
    },
    {
      "metadata": {
        "id": "zE6jg7HHThsk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YdMktgOMThsx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXelV3HjThtE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime, time, json\n",
        "from string import punctuation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "awzjplbcThtL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gMqAzlFThtO",
        "colab_type": "code",
        "outputId": "f96bfd7b-c28b-4c2e-badf-653efd119b7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "TOdeyLYEThtf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import initializers\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Dropout, Reshape, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
        "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DsJUOT3mThtx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_questions = sentences_1.tolist() + sentences_2.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gNE5S8VTht2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKbzxzKQTht5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "question1_word_sequences = tokenizer.texts_to_sequences(sentences_1.tolist())\n",
        "question2_word_sequences = tokenizer.texts_to_sequences(sentences_2.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjn1esYUThuK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WnTafrvlThuO",
        "colab_type": "code",
        "outputId": "f06842dd-9d57-447a-95ac-09e8f233cf4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "max_question_len = 0\n",
        "for each in range(length):\n",
        "    max_question_len = max(max_question_len, len(question1_word_sequences[each]), len(question2_word_sequences[each]))\n",
        "print(max_question_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i9zDir1ZThuh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_q1 = pad_sequences(question1_word_sequences,\n",
        "                              maxlen = max_question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0mG5A7_ThvP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_q2 = pad_sequences(question2_word_sequences,\n",
        "                              maxlen = max_question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bUJJYK2mThvV",
        "colab_type": "code",
        "outputId": "4bafc858-ad40-4a23-d6cd-9857bf40e422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open(common_crawl_embeddings, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 2196016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q1fCQe8jThvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qyRcPN4TThv3",
        "colab_type": "code",
        "outputId": "a4f34604-81aa-4a7d-bde8-1b91be2c343a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "nb_words = len(word_index)\n",
        "word_embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        word_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0)) #75,334"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 16082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "717pXiF4Thv8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "units = 128 # Number of nodes in the Dense layers\n",
        "dropout = 0.25 # Percentage of nodes to drop\n",
        "nb_filter = 32 # Number of filters to use in Convolution1D\n",
        "filter_length = 3 # Length of filter for Convolution1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtvBJ-IbThwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\n",
        "bias = bias_initializer='zeros'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfX5rFLjThwG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eWwGJL0sThwW",
        "colab_type": "code",
        "outputId": "535d7d83-d745-4074-8169-fa1431f65d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "model_1_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_1_input')\n",
        "model_1_embedding = Embedding(nb_words + 1,\n",
        "                     embedding_dim,\n",
        "                     weights = [word_embedding_matrix], \n",
        "                     input_length = max_question_len,\n",
        "                     trainable = False)(model_1_input)\n",
        "model_1_conv_a = Convolution1D(filters = nb_filter, \n",
        "                         kernel_size = filter_length, \n",
        "                         padding = 'same')(model_1_embedding)\n",
        "model_1_batch_a = BatchNormalization()(model_1_conv_a)\n",
        "model_1_act = Activation('relu')(model_1_batch_a)\n",
        "model_1_drop_a = Dropout(dropout)(model_1_act)\n",
        "model_1_conv_b = Convolution1D(filters = nb_filter, \n",
        "                         kernel_size = filter_length, \n",
        "                         padding = 'same')(model_1_drop_a)\n",
        "model_1_batch_b = BatchNormalization()(model_1_conv_b)\n",
        "model_1_act_b = Activation('relu')(model_1_batch_b)\n",
        "model_1_drop_b = Dropout(dropout)(model_1_act_b)\n",
        "model_1_flat = Flatten()(model_1_drop_b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RR3kfYpXThwm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_2_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_2_input')\n",
        "model_2_embedding = Embedding(nb_words + 1,\n",
        "                     embedding_dim,\n",
        "                     weights = [word_embedding_matrix], \n",
        "                     input_length = max_question_len,\n",
        "                     trainable = False)(model_2_input)\n",
        "model_2_conv_a = Convolution1D(filters = nb_filter, \n",
        "                         kernel_size = filter_length, \n",
        "                         padding = 'same')(model_2_embedding)\n",
        "model_2_batch_a = BatchNormalization()(model_2_conv_a)\n",
        "model_2_act = Activation('relu')(model_2_batch_a)\n",
        "model_2_drop_a = Dropout(dropout)(model_2_act)\n",
        "model_2_conv_b = Convolution1D(filters = nb_filter, \n",
        "                         kernel_size = filter_length, \n",
        "                         padding = 'same')(model_2_drop_a)\n",
        "model_2_batch_b = BatchNormalization()(model_2_conv_b)\n",
        "model_2_act_b = Activation('relu')(model_2_batch_b)\n",
        "model_2_drop_b = Dropout(dropout)(model_2_act_b)\n",
        "model_2_flat = Flatten()(model_2_drop_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6XGdYsi-Thwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_3_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_3_input')\n",
        "model_3_embedding = Embedding(nb_words + 1,\n",
        "                     embedding_dim,\n",
        "                     weights = [word_embedding_matrix],\n",
        "                     input_length = max_question_len,\n",
        "                     trainable = False)(model_3_input)\n",
        "model_3_time_distributed = TimeDistributed(Dense(embedding_dim))(model_3_embedding)\n",
        "model_3_batch = BatchNormalization()(model_3_time_distributed)\n",
        "model_3_act = Activation('relu')(model_3_batch)\n",
        "model_3_drop = Dropout(dropout)(model_3_act)\n",
        "model_3_lambda = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, ))(model_3_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZTqDf33Thwx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_4_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_4_input')\n",
        "model_4_embedding = Embedding(nb_words + 1,\n",
        "                     embedding_dim,\n",
        "                     weights = [word_embedding_matrix],\n",
        "                     input_length = max_question_len,\n",
        "                     trainable = False)(model_4_input)\n",
        "model_4_time_distributed = TimeDistributed(Dense(embedding_dim))(model_4_embedding)\n",
        "model_4_batch = BatchNormalization()(model_4_time_distributed)\n",
        "model_4_act = Activation('relu')(model_4_batch)\n",
        "model_4_drop = Dropout(dropout)(model_4_act)\n",
        "model_4_lambda = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, ))(model_4_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qzQZ3v0Thw2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWTYtVWfThw5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "merge_layer = concatenate([model_1_flat, model_2_flat, model_3_lambda, model_4_lambda], name = 'merge_layer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nhi18BHpThw_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = Dense(200, activation = 'relu', name = 'dense1')(merge_layer)\n",
        "t = Dropout(0.3)(t)\n",
        "t = BatchNormalization()(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H5gJrwBkThxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = Dense(200, activation = 'relu', name  ='dense2')(t)\n",
        "t = Dropout(0.3)(t)\n",
        "t = BatchNormalization()(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1UqIRnYQThxJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = Dense(100, activation= 'relu',name = 'dense3')(t)\n",
        "t = Dropout(0.3)(t)\n",
        "t = BatchNormalization()(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJMX7RW6ThxN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final_output = Dense(1, activation = 'sigmoid')(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IpEImjZlThxQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdB2f-2YThxU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs = [model_1_input, model_2_input, model_3_input, model_4_input], outputs = final_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SmThUDHkThxc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PbDmwHLThxy",
        "colab_type": "code",
        "outputId": "867fd578-8a8a-4d7e-f1ca-7361e7ea9259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4386
        }
      },
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "history = model.fit([train_q1, train_q2, train_q1, train_q2],\n",
        "                    questions.is_duplicate,\n",
        "                    batch_size=256,\n",
        "                    epochs=128, #Use 100, I reduce it for Kaggle,\n",
        "                    verbose=True,\n",
        "                    shuffle=True)\n",
        "t1 = time.time()\n",
        "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "404348/404348 [==============================] - 133s 328us/step - loss: 0.4278 - acc: 0.7913\n",
            "Epoch 2/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.4168 - acc: 0.7978\n",
            "Epoch 3/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.4066 - acc: 0.8036\n",
            "Epoch 4/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3979 - acc: 0.8096\n",
            "Epoch 5/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.3910 - acc: 0.8136\n",
            "Epoch 6/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3843 - acc: 0.8170\n",
            "Epoch 7/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.3773 - acc: 0.8212\n",
            "Epoch 8/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3726 - acc: 0.8245\n",
            "Epoch 9/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3681 - acc: 0.8269\n",
            "Epoch 10/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.3623 - acc: 0.8306\n",
            "Epoch 11/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3575 - acc: 0.8330\n",
            "Epoch 12/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.3548 - acc: 0.8336\n",
            "Epoch 13/128\n",
            "404348/404348 [==============================] - 130s 320us/step - loss: 0.3506 - acc: 0.8363\n",
            "Epoch 14/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3467 - acc: 0.8389\n",
            "Epoch 15/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3439 - acc: 0.8407\n",
            "Epoch 16/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3408 - acc: 0.8419\n",
            "Epoch 17/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3388 - acc: 0.8429\n",
            "Epoch 18/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.3343 - acc: 0.8453\n",
            "Epoch 19/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.3332 - acc: 0.8461\n",
            "Epoch 20/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.3301 - acc: 0.8478\n",
            "Epoch 21/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.3275 - acc: 0.8493\n",
            "Epoch 22/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3251 - acc: 0.8504\n",
            "Epoch 23/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3237 - acc: 0.8507\n",
            "Epoch 24/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3208 - acc: 0.8529\n",
            "Epoch 25/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.3189 - acc: 0.8537\n",
            "Epoch 26/128\n",
            "404348/404348 [==============================] - 130s 320us/step - loss: 0.3167 - acc: 0.8552\n",
            "Epoch 27/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3159 - acc: 0.8557\n",
            "Epoch 28/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.3132 - acc: 0.8574\n",
            "Epoch 29/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.3119 - acc: 0.8581\n",
            "Epoch 30/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.3102 - acc: 0.8588\n",
            "Epoch 31/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.3081 - acc: 0.8605\n",
            "Epoch 32/128\n",
            "404348/404348 [==============================] - 130s 323us/step - loss: 0.3071 - acc: 0.8602\n",
            "Epoch 33/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.3059 - acc: 0.8610\n",
            "Epoch 34/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.3039 - acc: 0.8618\n",
            "Epoch 35/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.3023 - acc: 0.8627\n",
            "Epoch 36/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.3011 - acc: 0.8631\n",
            "Epoch 37/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2992 - acc: 0.8642\n",
            "Epoch 38/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2979 - acc: 0.8651\n",
            "Epoch 39/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2963 - acc: 0.8658\n",
            "Epoch 40/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2961 - acc: 0.8661\n",
            "Epoch 41/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2950 - acc: 0.8662\n",
            "Epoch 42/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2924 - acc: 0.8679\n",
            "Epoch 43/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2926 - acc: 0.8678\n",
            "Epoch 44/128\n",
            "404348/404348 [==============================] - 130s 320us/step - loss: 0.2913 - acc: 0.8687\n",
            "Epoch 45/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2899 - acc: 0.8698\n",
            "Epoch 46/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2881 - acc: 0.8707\n",
            "Epoch 47/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2877 - acc: 0.8702\n",
            "Epoch 48/128\n",
            "404348/404348 [==============================] - 128s 318us/step - loss: 0.2859 - acc: 0.8715\n",
            "Epoch 49/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2865 - acc: 0.8715\n",
            "Epoch 50/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2846 - acc: 0.8726\n",
            "Epoch 51/128\n",
            "404348/404348 [==============================] - 128s 318us/step - loss: 0.2836 - acc: 0.8723\n",
            "Epoch 52/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2828 - acc: 0.8731\n",
            "Epoch 53/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2820 - acc: 0.8740\n",
            "Epoch 54/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2805 - acc: 0.8737\n",
            "Epoch 55/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2808 - acc: 0.8743\n",
            "Epoch 56/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2797 - acc: 0.8751\n",
            "Epoch 57/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2787 - acc: 0.8752\n",
            "Epoch 58/128\n",
            "404348/404348 [==============================] - 130s 320us/step - loss: 0.2779 - acc: 0.8756\n",
            "Epoch 59/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2764 - acc: 0.8761\n",
            "Epoch 60/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2763 - acc: 0.8764\n",
            "Epoch 61/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2754 - acc: 0.8773\n",
            "Epoch 62/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2752 - acc: 0.8774\n",
            "Epoch 63/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2730 - acc: 0.8779\n",
            "Epoch 64/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2737 - acc: 0.8774\n",
            "Epoch 65/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2718 - acc: 0.8788\n",
            "Epoch 66/128\n",
            "404348/404348 [==============================] - 129s 319us/step - loss: 0.2721 - acc: 0.8786\n",
            "Epoch 67/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.2701 - acc: 0.8794\n",
            "Epoch 68/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2702 - acc: 0.8798\n",
            "Epoch 69/128\n",
            "404348/404348 [==============================] - 131s 325us/step - loss: 0.2707 - acc: 0.8794\n",
            "Epoch 70/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2694 - acc: 0.8797\n",
            "Epoch 71/128\n",
            "404348/404348 [==============================] - 129s 319us/step - loss: 0.2685 - acc: 0.8803\n",
            "Epoch 72/128\n",
            "404348/404348 [==============================] - 129s 319us/step - loss: 0.2682 - acc: 0.8807\n",
            "Epoch 73/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2664 - acc: 0.8816\n",
            "Epoch 74/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2665 - acc: 0.8818\n",
            "Epoch 75/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2662 - acc: 0.8816\n",
            "Epoch 76/128\n",
            "404348/404348 [==============================] - 131s 325us/step - loss: 0.2653 - acc: 0.8820\n",
            "Epoch 77/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2647 - acc: 0.8823\n",
            "Epoch 78/128\n",
            "404348/404348 [==============================] - 131s 325us/step - loss: 0.2639 - acc: 0.8822\n",
            "Epoch 79/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2640 - acc: 0.8832\n",
            "Epoch 80/128\n",
            "404348/404348 [==============================] - 131s 325us/step - loss: 0.2627 - acc: 0.8834\n",
            "Epoch 81/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2628 - acc: 0.8832\n",
            "Epoch 82/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2626 - acc: 0.8834\n",
            "Epoch 83/128\n",
            "404348/404348 [==============================] - 130s 320us/step - loss: 0.2614 - acc: 0.8841\n",
            "Epoch 84/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2604 - acc: 0.8848\n",
            "Epoch 85/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2608 - acc: 0.8845\n",
            "Epoch 86/128\n",
            "404348/404348 [==============================] - 129s 320us/step - loss: 0.2602 - acc: 0.8844\n",
            "Epoch 87/128\n",
            "404348/404348 [==============================] - 129s 319us/step - loss: 0.2593 - acc: 0.8853\n",
            "Epoch 88/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2586 - acc: 0.8853\n",
            "Epoch 89/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2583 - acc: 0.8857\n",
            "Epoch 90/128\n",
            "404348/404348 [==============================] - 129s 319us/step - loss: 0.2580 - acc: 0.8864\n",
            "Epoch 91/128\n",
            "404348/404348 [==============================] - 131s 325us/step - loss: 0.2571 - acc: 0.8864\n",
            "Epoch 92/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2567 - acc: 0.8867\n",
            "Epoch 93/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2557 - acc: 0.8868\n",
            "Epoch 94/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2558 - acc: 0.8871\n",
            "Epoch 95/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2551 - acc: 0.8872\n",
            "Epoch 96/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2559 - acc: 0.8871\n",
            "Epoch 97/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2541 - acc: 0.8874\n",
            "Epoch 98/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2535 - acc: 0.8879\n",
            "Epoch 99/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2527 - acc: 0.8884\n",
            "Epoch 100/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2533 - acc: 0.8883\n",
            "Epoch 101/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2527 - acc: 0.8889\n",
            "Epoch 102/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2512 - acc: 0.8891\n",
            "Epoch 103/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2516 - acc: 0.8894\n",
            "Epoch 104/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2517 - acc: 0.8887\n",
            "Epoch 105/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2510 - acc: 0.8893\n",
            "Epoch 106/128\n",
            "404348/404348 [==============================] - 128s 316us/step - loss: 0.2503 - acc: 0.8900\n",
            "Epoch 107/128\n",
            "404348/404348 [==============================] - 128s 318us/step - loss: 0.2501 - acc: 0.8901\n",
            "Epoch 108/128\n",
            "404348/404348 [==============================] - 130s 320us/step - loss: 0.2505 - acc: 0.8892\n",
            "Epoch 109/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.2491 - acc: 0.8908\n",
            "Epoch 110/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2504 - acc: 0.8902\n",
            "Epoch 111/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.2478 - acc: 0.8912\n",
            "Epoch 112/128\n",
            "404348/404348 [==============================] - 130s 321us/step - loss: 0.2481 - acc: 0.8908\n",
            "Epoch 113/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2488 - acc: 0.8908\n",
            "Epoch 114/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2474 - acc: 0.8912\n",
            "Epoch 115/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2476 - acc: 0.8909\n",
            "Epoch 116/128\n",
            "404348/404348 [==============================] - 130s 323us/step - loss: 0.2466 - acc: 0.8915\n",
            "Epoch 117/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2467 - acc: 0.8917\n",
            "Epoch 118/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2457 - acc: 0.8922\n",
            "Epoch 119/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2459 - acc: 0.8921\n",
            "Epoch 120/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2461 - acc: 0.8916\n",
            "Epoch 121/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2460 - acc: 0.8917\n",
            "Epoch 122/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2448 - acc: 0.8923\n",
            "Epoch 123/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2445 - acc: 0.8928\n",
            "Epoch 124/128\n",
            "404348/404348 [==============================] - 130s 322us/step - loss: 0.2441 - acc: 0.8924\n",
            "Epoch 125/128\n",
            "404348/404348 [==============================] - 131s 323us/step - loss: 0.2437 - acc: 0.8926\n",
            "Epoch 126/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2433 - acc: 0.8932\n",
            "Epoch 127/128\n",
            "404348/404348 [==============================] - 131s 324us/step - loss: 0.2434 - acc: 0.8931\n",
            "Epoch 128/128\n",
            "404348/404348 [==============================] - 131s 325us/step - loss: 0.2431 - acc: 0.8930\n",
            "Minutes elapsed: 276.850619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BIQqiBB_ubfH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"cnn_td_quora_len_97.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vM3d_JDdrV8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "json = model.to_json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sKXjrEknrfy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"cnn_td_quora_len_97.json\", \"w\") as json_file:\n",
        "    json_file.write(json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WHFAzXzArfuT",
        "colab_type": "code",
        "outputId": "90bfe575-37c7-44a0-af72-03ec17762d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('cnn_td_quora_len_97.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1F4ONz6EEh-KojZTilrcjBsWXA89fvblq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dAAW4RkfsVnR",
        "colab_type": "code",
        "outputId": "55437778-b206-4cd2-8aaf-f1378f264cba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('cnn_td_quora_len_97.json')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 166gELhGcUW69T3NPxouzjQtoh8-QTvtB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zavl6-HazStz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del embeddings_index\n",
        "del embedding\n",
        "del model\n",
        "del train_q1\n",
        "del train_q2\n",
        "del word_embedding_matrix\n",
        "del question1_word_sequences\n",
        "del question2_word_sequences\n",
        "del all_questions\n",
        "del uploaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "elQTFbMuHhgN"
      },
      "cell_type": "markdown",
      "source": [
        "# Siamese Neural Networks(Using LSTM and GRU)"
      ]
    },
    {
      "metadata": {
        "id": "a1ln-4sTThyx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dE_9Xb7uHqgw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "import keras.backend as backend\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda, GRU, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWJ7T5-xThy_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lDqBuGauThzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q2n_left = []\n",
        "for sentence in sentences_1.tolist():\n",
        "    temp_sentence = []\n",
        "    for word in sentence.split():\n",
        "        if word not in vocabulary:\n",
        "            vocabulary[word] = len(inverse_vocabulary)\n",
        "            temp_sentence.append(len(inverse_vocabulary))\n",
        "            inverse_vocabulary.append(word)\n",
        "        else:\n",
        "            temp_sentence.append(vocabulary[word])\n",
        "    q2n_left.append(temp_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TUOuLqh0ThzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q2n_right = []\n",
        "for sentence in sentences_2.tolist():\n",
        "    temp_sentence = []\n",
        "    for word in sentence.split():\n",
        "        if word not in vocabulary:\n",
        "            vocabulary[word] = len(inverse_vocabulary)\n",
        "            temp_sentence.append(len(inverse_vocabulary))\n",
        "            inverse_vocabulary.append(word)\n",
        "        else:\n",
        "            temp_sentence.append(vocabulary[word])\n",
        "    q2n_right.append(temp_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d12cT2W6Thzh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "embeddings = np.zeros((len(vocabulary) + 1, embedding_dim))\n",
        "embeddings[0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m335l-nAGoh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UhtUrY3AGBCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "google_model = KeyedVectors.load_word2vec_format(google_embeddings, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GgJE5IG_Thz1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for word, index in vocabulary.items():\n",
        "    if word in google_model.vocab:\n",
        "        embeddings[index] = google_model.word_vec(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3yylXEJPThz5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del google_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpjEmyWSTh0H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6cdNIb5rTh0K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_left = q2n_left"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c6xYqZ0CTh0O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_right = q2n_right"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nx1ohcwUTh0Y",
        "colab_type": "code",
        "outputId": "82e15527-b426-46d9-fb94-c0df6b85a021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "max_seq_length = 0\n",
        "for each in range(length):\n",
        "    max_seq_length = max(max_seq_length, len(q2n_left[each]), len(q2n_right[each]))\n",
        "print(max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a0A4__Z7Th0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_left = pad_sequenceshttps://colab.research.google.com/drive/1uwFl2clpz12Sf5toLFl_bcWoB1r0DKGi?authuser=4#(q2n_left, maxlen=max_seq_length)\n",
        "dataset_right = pad_sequences(q2n_right, maxlen=max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0tRbrmVTh0v",
        "colab_type": "code",
        "outputId": "5993442f-ace2-416f-de67-58e9441ff2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dataset_left.shape == dataset_right.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "mv21EhXoTh1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_hidden1 = 512\n",
        "n_hidden2 = 384\n",
        "n_hidden3 = 256\n",
        "n_hidden4 = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WljRtYIvJcTn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "left_input = Input(shape=(max_seq_length, ), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length, ), dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ksWe8MukKbxW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], \n",
        "                            input_length=max_seq_length, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U0jTBGv4Kck8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5e9B1ChNKgA3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_lstm1 = LSTM(n_hidden1, return_sequences=True)\n",
        "shared_dropout1 = Dropout(0.3)\n",
        "shared_gru1 = GRU(n_hidden2, return_sequences=True)\n",
        "shared_dropout2 = Dropout(0.4)\n",
        "shared_gru2 = GRU(n_hidden3, return_sequences=True)\n",
        "shared_dropout3 = Dropout(0.3)\n",
        "shared_lstm2 = LSTM(n_hidden4, return_sequences=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oiSpiz9uLGu3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "left_lstm1 = shared_lstm1(encoded_left)\n",
        "left_dropout1 = shared_dropout1(left_lstm1)\n",
        "left_gru1 = shared_gru1(left_dropout1)\n",
        "left_dropout2 = shared_dropout2(left_gru1)\n",
        "left_gru2 = shared_gru2(left_dropout2)\n",
        "left_dropout3 = shared_dropout3(left_gru2)\n",
        "left_lstm2 = shared_lstm2(left_dropout3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bcQJfYn6Ma5S",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "right_lstm1 = shared_lstm1(encoded_right)\n",
        "right_dropout1 = shared_dropout1(right_lstm1)\n",
        "right_gru1 = shared_gru1(right_dropout1)\n",
        "right_dropout2 = shared_dropout2(right_gru1)\n",
        "right_gru2 = shared_gru2(right_dropout2)\n",
        "right_dropout3 = shared_dropout3(right_gru2)\n",
        "right_lstm2 = shared_lstm2(right_dropout3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Ug1R5gTGM4oR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "manhattan_distance_for_lstm = Lambda(function=lambda x: backend.exp(-backend.sum(backend.abs(x[0]-x[1]), axis=1, keepdims=True)),\n",
        "                                     output_shape=lambda x: (x[0][0], 1))([left_lstm2, right_lstm2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "n5TGTK2CUrN8",
        "outputId": "a31d7321-4ac4-4a1e-f1ee-9f240a331357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "siamese_network = Model([left_input, right_input], manhattan_distance_for_lstm)\n",
        "siamese_network.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=['accuracy'])\n",
        "siamese_network.fit([dataset_left, dataset_right], is_duplicate_questions, batch_size=256, \n",
        "                        epochs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "404348/404348 [==============================] - 3060s 8ms/step - loss: 0.1753 - acc: 0.7357\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2de92455f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "w2i2SErFIcrS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "siamese_network.save_weights(\"siamese_network_quora_len_97.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iCYLJFBq_m1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('siamese_network_quora_len_97.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oytgyy-eYs6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "json_file = siamese_network.to_json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40V_IYlO_oME",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"siamese_network_quora_len_97.json\", \"w\") as json:\n",
        "    json_file.write(json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RwglWk8QLtcu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('siamese_network_quora_len_97.json')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjtfAYBSw6F3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del vocabulary\n",
        "del inverse_vocabulary\n",
        "del q2n_left\n",
        "del q2n_right\n",
        "del embedding_dim\n",
        "del embeddings\n",
        "del dataset_left\n",
        "del dataset_right\n",
        "del siamese_network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPmU23LBGNe1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Siamese Network of CNN, Time Distributed, RNN and Attention Model"
      ]
    },
    {
      "metadata": {
        "id": "U_Q6kDe9PM4S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZ3FAytOPLbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers as initializers, regularizers, constraints\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, Lambda, TimeDistributed, Convolution1D, Flatten, Activation, BatchNormalization, Dropout\n",
        "from keras import backend as K\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U6VzQKGWQTXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_questions = sentences_1.tolist() + sentences_2.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YbjmDqxQdC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FheV4es1Qi-A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "question1_word_sequences = tokenizer.texts_to_sequences(sentences_1.tolist())\n",
        "question2_word_sequences = tokenizer.texts_to_sequences(sentences_2.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xSlitiLuQo50",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vWx6owG0SsIT",
        "colab_type": "code",
        "outputId": "05323ef6-fecb-467c-e200-40109c09cd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "max_question_len = 0\n",
        "for each in range(length):\n",
        "    max_question_len = max(max_question_len, len(question1_word_sequences[each]), len(question2_word_sequences[each]))\n",
        "print(max_question_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qJUib0e9Y48U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4aTJr1ilS2jR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_q1 = pad_sequences(question1_word_sequences,\n",
        "                              maxlen = max_question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QWCl6OqHS5ZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_q2 = pad_sequences(question2_word_sequences,\n",
        "                              maxlen = max_question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNDBXXRyTRzZ",
        "colab_type": "code",
        "outputId": "693a8c24-1a8c-485b-da81-50bcab25ddce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open('common-crawl-embeddings-w2v.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 2196016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qhkHdnIqTZxJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uti-gVr_Ta0k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_words = len(word_index)\n",
        "word_embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        word_embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bjo3A6TLU13W",
        "colab_type": "code",
        "outputId": "aae47773-9372-4fe2-894b-9b25e46da0f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0)) #75,334"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 16082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z3XqT4_mMCgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "    \n",
        "    def dot_product(self, x, kernel):\n",
        "        if K.backend() == 'tensorflow':\n",
        "            return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "        else:\n",
        "            return K.dot(x, kernel)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = self.dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = self.dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRhzr047Gm5n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "left_input = Input(shape=(max_question_len, ), dtype='int32')\n",
        "right_input = Input(shape=(max_question_len, ), dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TcKGdGCQGtAg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_embedding_layer = Embedding(nb_words + 1, embedding_dim,\n",
        "                                   weights = [word_embedding_matrix], \n",
        "                                   input_length = max_question_len,\n",
        "                                   trainable = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "obKRVrQpzFV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "units = 128 # Number of nodes in the Dense layers\n",
        "dropout = 0.25 # Percentage of nodes to drop\n",
        "nb_filter = 128 # Number of filters to use in Convolution1D\n",
        "filter_length = 3 # Length of filter for Convolution1D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1ZQTlgeG8OP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_conv_layer = Convolution1D(filters = nb_filter, \n",
        "                            kernel_size = filter_length,\n",
        "                            padding = 'same')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R4jDcplvJcnr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_dropout_1 = Dropout(dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0KhJ2JtyIT2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_batch_normalization_layer = BatchNormalization()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FsNfIVfpHYwW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_activation_layer = TimeDistributed(Dense(128, activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnivfMrUIZRB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_time_distributed_dense_layer = TimeDistributed(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "81IW9wmiKvgI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_bidirectional_lstm_layer = Bidirectional(LSTM(units, return_sequences=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EefmwS4kJjpm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_dropout_2 = Dropout(dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "efQb0ZIwLGxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_bidirectional_gru_layer = Bidirectional(GRU(units, return_sequences=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XlyumzZaJmCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_dropout_3 = Dropout(dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-Vb35i7LUP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_attention_layer = AttentionWithContext()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ID33W_KYp1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Siamese Network**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ycBaJ0ztuVKR",
        "outputId": "cce28f6d-0e2a-422e-cd90-84b57f91028c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "embedded_left = shared_embedding_layer(left_input)\n",
        "embedded_right = shared_embedding_layer(right_input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0I4ZnPe4uXin",
        "colab_type": "code",
        "outputId": "eb6ecc5a-2484-41b8-f60e-877a5671bb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "cnn_left = shared_conv_layer(embedded_left)\n",
        "dropout_1_left = shared_dropout_1(cnn_left)\n",
        "batch_norm_left = shared_batch_normalization_layer(dropout_1_left)\n",
        "activation_left = shared_activation_layer(batch_norm_left)\n",
        "time_distributed_dense_left = shared_time_distributed_dense_layer(activation_left)\n",
        "bidirectional_lstm_left = shared_bidirectional_lstm_layer(time_distributed_dense_left)\n",
        "dropout_2_left = shared_dropout_2(bidirectional_lstm_left)\n",
        "bidirectional_gru_left = shared_bidirectional_gru_layer(dropout_2_left)\n",
        "dropout_3_left = shared_dropout_3(bidirectional_gru_left)\n",
        "attention_left = shared_attention_layer(dropout_3_left)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wAhRLP8Vvt92",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_right = shared_conv_layer(embedded_right)\n",
        "dropout_1_right = shared_dropout_1(cnn_right)\n",
        "batch_norm_right = shared_batch_normalization_layer(cnn_right)\n",
        "activation_right = shared_activation_layer(batch_norm_right)\n",
        "time_distributed_dense_right = shared_time_distributed_dense_layer(activation_right)\n",
        "bidirectional_lstm_right = shared_bidirectional_lstm_layer(time_distributed_dense_right)\n",
        "dropout_2_right = shared_dropout_2(bidirectional_lstm_right)\n",
        "bidirectional_gru_right = shared_bidirectional_gru_layer(dropout_2_right)\n",
        "dropout_3_right = shared_dropout_3(bidirectional_gru_right)\n",
        "attention_right = shared_attention_layer(dropout_3_right)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xDWcW5bu15sf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_layer = Lambda(function=lambda x: K.exp(-K.sum(K.abs(x[0]-x[1]), axis=1, keepdims=True)),\n",
        "                                     output_shape=lambda x: (x[0][0], 1))([attention_left, attention_right])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5b_z6oqwB-3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model([left_input, right_input], output_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J74Pl1rX2LdD",
        "colab_type": "code",
        "outputId": "256a3f84-3168-468f-8c32-e325f3d31a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 97)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 97, 300)      19608000    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 97)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 97, 128)      115328      embedding_1[0][0]                \n",
            "                                                                 embedding_1[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 97, 128)      0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 97, 128)      512         dropout_1[0][0]                  \n",
            "                                                                 conv1d_1[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 97, 128)      16512       batch_normalization_1[0][0]      \n",
            "                                                                 batch_normalization_1[1][0]      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 97, 128)      0           time_distributed_1[0][0]         \n",
            "                                                                 time_distributed_1[1][0]         \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 97, 256)      263168      time_distributed_2[0][0]         \n",
            "                                                                 time_distributed_2[1][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 97, 256)      0           bidirectional_1[0][0]            \n",
            "                                                                 bidirectional_1[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 97, 256)      295680      dropout_2[0][0]                  \n",
            "                                                                 dropout_2[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 97, 256)      0           bidirectional_2[0][0]            \n",
            "                                                                 bidirectional_2[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "attention_with_context_1 (Atten (None, 256)          66048       dropout_3[0][0]                  \n",
            "                                                                 dropout_3[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1)            0           attention_with_context_1[0][0]   \n",
            "                                                                 attention_with_context_1[1][0]   \n",
            "==================================================================================================\n",
            "Total params: 20,365,248\n",
            "Trainable params: 756,992\n",
            "Non-trainable params: 19,608,256\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TkLHTUDLwYcl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGq2mHgQwbCi",
        "colab_type": "code",
        "outputId": "5832a777-715d-4399-929e-611ad8a6d07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit([train_q1, train_q2], is_duplicate_questions, batch_size=256, \n",
        "                        epochs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "404348/404348 [==============================] - 2525s 6ms/step - loss: 0.1705 - acc: 0.7443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc8053ed588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "9H2E71rb-Kyx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights('super_siamese_quora_len_97.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrYaHQby2Zow",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('super_siamese_quora_len_97.h5')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BCGCAN4BiXgX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "json_file = siamese_network.to_json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wMlfUjdJiXUu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"super_siamese_quora_len_97.json\", \"w\") as json:\n",
        "    json_file.write(json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NEJ3rBeMimoj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = drive.CreateFile()\n",
        "uploaded.SetContentFile('super_siamese_quora_len_97.json')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lE2P-_eSxEhI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del embeddings_index\n",
        "del embedding\n",
        "del model\n",
        "del train_q1\n",
        "del train_q2\n",
        "del word_embedding_matrix\n",
        "del question1_word_sequences\n",
        "del question2_word_sequences\n",
        "del all_questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nk8iOwrFRwn8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ensembling"
      ]
    },
    {
      "metadata": {
        "id": "56tVSd5srjL6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Dataset Creation**"
      ]
    },
    {
      "metadata": {
        "id": "eh9LdtnFrpQT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_scores = pd.DataFrame(X_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJyRXzh5xMsC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_scores.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XdVP-6-Royxk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Genetic Algorithm**"
      ]
    },
    {
      "metadata": {
        "id": "TmMBnIe80MJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/hyperparameter-tuning-in-xgboost-using-genetic-algorithm-17bd2e581b17"
      ]
    },
    {
      "metadata": {
        "id": "_U_8PY_HiH4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKdSZaa8iPH7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(723)\n",
        "np.random.seed(723)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xLaYUzqiWlt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initialization**"
      ]
    },
    {
      "metadata": {
        "id": "OPiOxOqiiP8N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initilialize_poplulation(numberOfParents):\n",
        "    learningRate = np.empty([numberOfParents, 1])\n",
        "    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
        "    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
        "    minChildWeight = np.empty([numberOfParents, 1])\n",
        "    gammaValue = np.empty([numberOfParents, 1])\n",
        "    subSample = np.empty([numberOfParents, 1])\n",
        "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
        "\n",
        "    for i in range(numberOfParents):\n",
        "        print(i)\n",
        "        learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
        "        nEstimators[i] = random.randrange(10, 1500, step = 25)\n",
        "        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n",
        "        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n",
        "        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
        "        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
        "        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
        "    \n",
        "    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n",
        "    return population"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8WIRcpoihHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Parent selection (survival of the fittest)**"
      ]
    },
    {
      "metadata": {
        "id": "khnxonLCjG0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create fitness function that will predict F1_score\n",
        "def fitness_f1score(y_true, y_pred):\n",
        "    fitness = round((f1_score(y_true, y_pred, average='weighted')), 4)\n",
        "    return fitness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nWo_OT0-inTL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train the data annd find fitness score\n",
        "def train_population(population, dMatrixTrain, dMatrixtest, y_test):\n",
        "    fScore = []\n",
        "    for i in range(population.shape[0]):\n",
        "        param = { 'objective':'binary:logistic',\n",
        "              'learning_rate': population[i][0],\n",
        "              'n_estimators': population[i][1], \n",
        "              'max_depth': int(population[i][2]), \n",
        "              'min_child_weight': population[i][3],\n",
        "              'gamma': population[i][4], \n",
        "              'subsample': population[i][5],\n",
        "              'colsample_bytree': population[i][6],\n",
        "              'seed': 24}\n",
        "        num_round = 100\n",
        "        xgbT = xgb.train(param, dMatrixTrain, num_round)\n",
        "        preds = xgbT.predict(dMatrixtest)\n",
        "        preds = preds>0.5\n",
        "        fScore.append(fitness_f1score(y_test, preds))\n",
        "    return fScore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HI6i1UPejEAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#select parents for mating\n",
        "def new_parents_selection(population, fitness, numParents):\n",
        "    selectedParents = np.empty((numParents, population.shape[1])) #create an array to store fittest parents\n",
        "    \n",
        "    #find the top best performing parents\n",
        "    for parentId in range(numParents):\n",
        "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
        "        bestFitnessId  = bestFitnessId[0][0]\n",
        "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
        "        fitness[bestFitnessId] = -1 #set this value to negative, in case of F1-score, so this parent is not selected again\n",
        "    return selectedParents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tacDj-0iwx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Crossover**"
      ]
    },
    {
      "metadata": {
        "id": "wG2fG-CHixYv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Mate these parents to create chilren having parameters from these parents (we are using uniform crossover method)\n",
        "'''\n",
        "def crossover_uniform(parents, childrenSize):\n",
        "    \n",
        "    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) #get all the index\n",
        "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) # select half  of the indexes randomly\n",
        "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes\n",
        "    \n",
        "    children = np.empty(childrenSize)\n",
        "    \n",
        "    '''\n",
        "    Create child by choosing parameters from two paraents selected using new_parent_selection function. The parameter values\n",
        "    will be picked from the indexes, which were randomly selected above. \n",
        "    '''\n",
        "    for i in range(childrenSize[0]):\n",
        "        \n",
        "        #find parent 1 index \n",
        "        parent1_index = i%parents.shape[0]\n",
        "        #find parent 2 index\n",
        "        parent2_index = (i+1)%parents.shape[0]\n",
        "        #insert parameters based on random selected indexes in parent 1\n",
        "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
        "        #insert parameters based on random selected indexes in parent 1\n",
        "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
        "    return children\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCLjuiYmjUYJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Mutation**"
      ]
    },
    {
      "metadata": {
        "id": "W-VEJrDpjV7u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Introduce some mutation in the children. In case of XGboost we will introdcue mutation randomly on each parameter one at a time,\n",
        "based on which parameter is selected at random. Initially, we will define the maximum/minimum value that is allowed for the parameter, to prevent the\n",
        "out the range error during runtime. Subsequently, we will generate mutation value and add it to the parameter, and return the mutated offspring!!!\n",
        "'''\n",
        "\n",
        "def mutation(crossover, numberOfParameters):\n",
        "    #Define minimum and maximum values allowed for each parameter\n",
        "\n",
        "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
        "    \n",
        "    minMaxValue[0:] = [0.01, 1.0] #min/max learning rate\n",
        "    minMaxValue[1, :] = [10, 2000] #min/max n_estimator\n",
        "    minMaxValue[2, :] = [1, 15] #min/max depth\n",
        "    minMaxValue[3, :] = [0, 10.0] #min/max child_weight\n",
        "    minMaxValue[4, :] = [0.01, 10.0] #min/max gamma\n",
        "    minMaxValue[5, :] = [0.01, 1.0] #min/maxsubsample\n",
        "    minMaxValue[6, :] = [0.01, 1.0] #min/maxcolsample_bytree\n",
        " \n",
        "    # Mutation changes a single gene in each offspring randomly.\n",
        "    mutationValue = 0\n",
        "    parameterSelect = np.random.randint(0, 7, 1)\n",
        "    print(parameterSelect)\n",
        "    if parameterSelect == 0: #learning_rate\n",
        "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
        "    if parameterSelect == 1: #n_estimators\n",
        "        mutationValue = np.random.randint(-200, 200, 1)\n",
        "    if parameterSelect == 2: #max_depth\n",
        "        mutationValue = np.random.randint(-5, 5, 1)\n",
        "    if parameterSelect == 3: #min_child_weight\n",
        "        mutationValue = round(np.random.uniform(5, 5), 2)\n",
        "    if parameterSelect == 4: #gamma\n",
        "        mutationValue = round(np.random.uniform(-2, 2), 2)\n",
        "    if parameterSelect == 5: #subsample\n",
        "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
        "    if parameterSelect == 6: #colsample\n",
        "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
        "  \n",
        "    #indtroduce mutation by changing one parameter, and set to max or min if it goes out of range\n",
        "    for idx in range(crossover.shape[0]):\n",
        "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
        "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
        "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
        "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
        "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n",
        "    return crossover"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dTqZEOq8jjCi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualization of Genetic Algorithm Performance**"
      ]
    },
    {
      "metadata": {
        "id": "3khpoUpojp3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "This function will allow us to genrate the heatmap for various parameters and fitness to visualize \n",
        "how each parameter and fitness changes with each generation\n",
        "'''\n",
        "\n",
        "def plot_parameters(numberOfGenerations, numberOfParents, parameter, parameterName):\n",
        "    #inspired from https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
        "    generationList = [\"Gen {}\".format(i) for i in range(numberOfGenerations+1)]\n",
        "    populationList = [\"Parent {}\".format(i) for i in range(numberOfParents)]\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(parameter, cmap=plt.get_cmap('YlOrBr'))\n",
        "    \n",
        "    # show ticks\n",
        "    ax.set_xticks(np.arange(len(populationList)))\n",
        "    ax.set_yticks(np.arange(len(generationList)))\n",
        "    \n",
        "    # show labels\n",
        "    ax.set_xticklabels(populationList)\n",
        "    ax.set_yticklabels(generationList)\n",
        "    \n",
        "    # set ticks at 45 degrees and rotate around anchor\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "    \n",
        "    \n",
        "    # insert the value of the parameter in each cell\n",
        "    for i in range(len(generationList)):\n",
        "        for j in range(len(populationList)):\n",
        "            text = ax.text(j, i, parameter[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"k\")\n",
        "    \n",
        "    ax.set_title(\"Change in the value of \" + parameterName)\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b4wCblAvjrz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Xgboost**"
      ]
    },
    {
      "metadata": {
        "id": "3O7R7IJ9j_Su",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qh417p30kY05",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = X_scores.iloc[:, :].values\n",
        "y = is_duplicate_questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I37MwwVcmAgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 97)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZzu24UakJoc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xgDMatrix = xgb.DMatrix(X_train, y_train)\n",
        "xgbDMatrixTest = xgb.DMatrix(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6o6aa1uEmKJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Let's find optimized parameters using genetic algorithms\n",
        "'''\n",
        "\n",
        "numberOfParents = 8 #number of parents to start\n",
        "numberOfParentsMating = 4 #number of parents that will mate\n",
        "numberOfParameters = 7 #number of parameters that will be optimized\n",
        "numberOfGenerations = 4 #number of genration that will be created"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2brclSh0mPFS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define the population size\n",
        "populationSize = (numberOfParents, numberOfParameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmoljjhqmZjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize the population with randomly generated parameters\n",
        "population = initilialize_poplulation(numberOfParents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7-7bKTDmT7N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#define an array to store the fitness  hitory\n",
        "fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFDZLnW5mT3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#insert the value of initial parameters to history\n",
        "populationHistory[0:numberOfParents, :] = population"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UPQOj7rAmTpT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for generation in range(numberOfGenerations):\n",
        "    print(\"This is number %s generation\" % (generation))\n",
        "    \n",
        "    #train the dataset and obtain fitness\n",
        "    fitnessValue = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
        "    fitnessHistory[generation, :] = fitnessValue\n",
        "    \n",
        "    #best score in the current iteration\n",
        "    print('Best F1 score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n",
        "\n",
        "    #survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n",
        "    parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n",
        "    \n",
        "    #mate these parents to create children having parameters from these parents (we are using uniform crossover)\n",
        "    children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n",
        "    \n",
        "    #add mutation to create genetic diversity\n",
        "    children_mutated = mutation(children, numberOfParameters)\n",
        "    \n",
        "    '''\n",
        "    We will create new population, which will contain parents that where selected previously based on the\n",
        "    fitness score and rest of them  will be children\n",
        "    '''\n",
        "    population[0:parents.shape[0], :] = parents #fittest parents\n",
        "    population[parents.shape[0]:, :] = children_mutated #children\n",
        "    \n",
        "    populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population #srore parent information"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKSUF7ZCmpVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Best solution from the final iteration\n",
        "fitness = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
        "fitnessHistory[generation+1, :] = fitness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ONlfFI3cmsH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#index of the best solution\n",
        "bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IRxCa5k6mugn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Best fitness\n",
        "print(\"Best fitness is =\", fitness[bestFitnessIndex])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLhivE2LmxF6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Best parameters\n",
        "print(\"Best parameters are:\")\n",
        "print('learning_rate', population[bestFitnessIndex][0])\n",
        "print('n_estimators', population[bestFitnessIndex][1])\n",
        "print('max_depth', int(population[bestFitnessIndex][2])) \n",
        "print('min_child_weight', population[bestFitnessIndex][3])\n",
        "print('gamma', population[bestFitnessIndex][4])\n",
        "print('subsample', population[bestFitnessIndex][5])\n",
        "print('colsample_bytree', population[bestFitnessIndex][6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "70DD9ckMm72C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#visualize the change in fitness of the various generations and parents\n",
        "plot_parameters(numberOfGenerations, numberOfParents, fitnessHistory, \"fitness (F1-score)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJyKhKZum_SF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Look at individual parameters change with generation\n",
        "#Create array for each parameter history (Genration x Parents)\n",
        "learnigRateHistory = populationHistory[:, 0].reshape([numberOfGenerations+1, numberOfParents])\n",
        "nEstimatorHistory = populationHistory[:, 1].reshape([numberOfGenerations+1, numberOfParents])\n",
        "maxdepthHistory = populationHistory[:, 2].reshape([numberOfGenerations+1, numberOfParents])\n",
        "minChildWeightHistory = populationHistory[:, 3].reshape([numberOfGenerations+1, numberOfParents])\n",
        "gammaHistory = populationHistory[:, 4].reshape([numberOfGenerations+1, numberOfParents])\n",
        "subsampleHistory = populationHistory[:, 5].reshape([numberOfGenerations+1, numberOfParents])\n",
        "colsampleByTreeHistory = populationHistory[:, 6].reshape([numberOfGenerations+1, numberOfParents])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "536N8XppnCPf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#generate heatmap for each parameter\n",
        "plot_parameters(numberOfGenerations, numberOfParents, learnigRateHistory, \"learning rate\")\n",
        "plot_parameters(numberOfGenerations, numberOfParents, nEstimatorHistory, \"n_estimator\")\n",
        "plot_parameters(numberOfGenerations, numberOfParents, maxdepthHistory, \"maximum depth\")\n",
        "plot_parameters(numberOfGenerations, numberOfParents, minChildWeightHistory, \"minimum child weight\")\n",
        "plot_parameters(numberOfGenerations, numberOfParents, gammaHistory, \"gamma\")\n",
        "plot_parameters(numberOfGenerations, numberOfParents, subsampleHistory, \"subsample\")\n",
        "plot_parameters(numberOfGenerations, numberOfParents, colsampleByTreeHistory, \"col sample by history\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}