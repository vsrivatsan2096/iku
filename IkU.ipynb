{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3hUGck-ksJp"
   },
   "source": [
    "# Semantics Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPXZlsook9cd"
   },
   "source": [
    "**Installing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqGfhdAakih2"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q numpy\n",
    "!pip install -U -q keras\n",
    "!pip install -U -q scikit-learn\n",
    "!pip install -U -q matplotlib\n",
    "!pip install -U -q nltk\n",
    "!pip install -U -q PyDrive \n",
    "!pip install -U -q pandas\n",
    "!pip3 install --quiet tensorflow-hub\n",
    "!pip3 install --quiet seaborn\n",
    "!pip3 install --quiet \"tensorflow>=1.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "016VNjCml2NW"
   },
   "source": [
    "**Getting data from Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TEWzFSplqVP"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSqhf7dHl_SS"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqJZtt-HmF6v"
   },
   "outputs": [],
   "source": [
    "file_ids = [\"16-aKOfyeLQpBJlUHCJUGxWp4UsY2rvb3\", \"1oec77bHzg5a2oGshDuBe99jxMi80NlUo\"]\n",
    "file_names = [\"train_translated.csv\", \"test_translated.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLtygnFNmLx_"
   },
   "outputs": [],
   "source": [
    "for each_id, each_name in zip(file_ids, file_names):\n",
    "    download = drive.CreateFile({'id':each_id})\n",
    "    download.GetContentFile(each_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNPMenzA0Yyj"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3Z7qyzq0g6Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\"E:\\Datasets\\quora\\questions.csv\")\n",
    "questions.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions1 = questions.iloc[:, 3].values\n",
    "questions2 = questions.iloc[:, 4].values\n",
    "is_duplicate_questions = questions.iloc[:, 5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjk1awi9vns5"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNVsrA9WHe5b"
   },
   "outputs": [],
   "source": [
    "length = is_duplicate_questions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_p_l_rms_l_1 = questions1\n",
    "dataset_p_l_rms_l_2 = questions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7q4ZLpcKj0v"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VsMTUtGdKoJM"
   },
   "source": [
    "**Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TEybJIOKkgT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "uRnXzsycKsMg",
    "outputId": "8b41cc96-5296-44c5-ca5a-9d62266da722",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vsriv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vsriv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTPkxZdiK0BD"
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "stopword = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd75AVttK7hD"
   },
   "outputs": [],
   "source": [
    "dataset_p_l_rms_l_1 = []\n",
    "dataset_p_l_rms_1 = []\n",
    "for i in questions1:\n",
    "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
    "    tempx = tempx.lower().split()\n",
    "    tempx = [word for word in tempx if word not in stopword]\n",
    "    dataset_p_l_rms_1.append(\" \".join(tempx))\n",
    "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
    "    dataset_p_l_rms_l_1.append(\" \".join(tempx))\n",
    "dataset_p_l_rms_l_1 = np.asarray(dataset_p_l_rms_l_1)\n",
    "dataset_p_l_rms_1 = np.asarray(dataset_p_l_rms_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdAWSHcJHe5q"
   },
   "outputs": [],
   "source": [
    "dataset_p_l_rms_l_2 = []\n",
    "dataset_p_l_rms_2 = []\n",
    "for i in questions2:\n",
    "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
    "    tempx = tempx.lower().split()\n",
    "    tempx = [word for word in tempx if word not in stopword]\n",
    "    dataset_p_l_rms_2.append(\" \".join(tempx))\n",
    "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
    "    dataset_p_l_rms_l_2.append(\" \".join(tempx))\n",
    "dataset_p_l_rms_l_2 = np.asarray(dataset_p_l_rms_l_2)\n",
    "dataset_p_l_rms_2 = np.asarray(dataset_p_l_rms_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astrology capricorn sun cap moon cap rising say\n",
      "triple capricorn sun moon ascendant capricorn say\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset_no = 5\n",
    "print(dataset_p_l_rms_1[dataset_no])\n",
    "print(dataset_p_l_rms_2[dataset_no])\n",
    "print(is_duplicate_questions[dataset_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorizor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.fit(np.append(dataset_p_l_rms_l_1, dataset_p_l_rms_l_2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dataset_p_l_rms_l_1 = count_vectorizer.transform(dataset_p_l_rms_l_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dataset_p_l_rms_l_2 = count_vectorizer.transform(dataset_p_l_rms_l_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_and_similarity_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(v_dataset_p_l_rms_l_1[i], v_dataset_p_l_rms_l_2[i])[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(v_dataset_p_l_rms_l_1[i], v_dataset_p_l_rms_l_2[i])[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(v_dataset_p_l_rms_l_1[i], v_dataset_p_l_rms_l_2[i])[0][0]\n",
    "    distance_and_similarity_scores.append(temp)\n",
    "#try to give this to log reg to find the similarity between these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tfidf Vectorizor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.fit(np.append(dataset_p_l_rms_l_1, dataset_p_l_rms_l_2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_dataset_p_l_rms_l_1 = tfidf_vectorizer.transform(dataset_p_l_rms_l_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_dataset_p_l_rms_l_2 = tfidf_vectorizer.transform(dataset_p_l_rms_l_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(V_dataset_p_l_rms_l_1[i], V_dataset_p_l_rms_l_2[i])[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(V_dataset_p_l_rms_l_1[i], V_dataset_p_l_rms_l_2[i])[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(V_dataset_p_l_rms_l_1[i], V_dataset_p_l_rms_l_2[i])[0][0]\n",
    "    distance_and_similarity_scores_2.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-xcGFtPGdfJ"
   },
   "source": [
    "**LSA Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYXzCvrQGgm3"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHMV-1Bhai1e"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DObvs6mal97"
   },
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=300,\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__L-nT-lapFP"
   },
   "outputs": [],
   "source": [
    "lsa_model1 = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCFrhTD-He55"
   },
   "outputs": [],
   "source": [
    "lsa_model2 = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0Qm7cU-asMM"
   },
   "outputs": [],
   "source": [
    "lsa_test1 = lsa_model1.fit_transform(dataset_p_l_rms_l_1)\n",
    "lsa_test2 = lsa_model2.fit_transform(dataset_p_l_rms_l_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_dataset_p_l_rms_l_1 = lsa_test1\n",
    "lsa_dataset_p_l_rms_l_2 = lsa_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShsDeBj0He59"
   },
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-RqCVPvHe5_"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(np.asarray([lsa_dataset_p_l_rms_l_1[i]]), np.asarray([lsa_dataset_p_l_rms_l_2[i]]))[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(np.asarray([lsa_dataset_p_l_rms_l_1[i]]), np.asarray([lsa_dataset_p_l_rms_l_2[i]]))[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(np.asarray([lsa_dataset_p_l_rms_l_1[i]]), np.asarray([lsa_dataset_p_l_rms_l_2[i]]))[0][0]\n",
    "    distance_and_similarity_scores_3.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHVZTTRYHe6A",
    "outputId": "cc91ef3d-720c-4269-f4f8-c6729caa3889"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_3[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uENvSaywLgrP"
   },
   "source": [
    "**Word2Vec model(Using Mean to get the sentence vectors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_vSYQP8LN36"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61vPjXTNB8Ur"
   },
   "outputs": [],
   "source": [
    "#wiki_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/wiki/wiki.300d.txt\", binary=False)\n",
    "google_model = KeyedVectors.load_word2vec_format(\"E:\\Models\\pre_trained\\word2vec\\google\\google.300d.bin\", binary=True)\n",
    "#common_crawl_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/common_crawl/common_crawl.300d.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOUcqYwzLzTf"
   },
   "outputs": [],
   "source": [
    "def sentence_vectorizer(model, sentence):\n",
    "    vectors =[]\n",
    "    num = 0\n",
    "    for i in sentence.split():\n",
    "        try:\n",
    "            if num == 0:\n",
    "                vectors = model[i]\n",
    "            else:\n",
    "                vectors = np.add(vectors, model[i])\n",
    "            num += 1\n",
    "        except:\n",
    "            pass\n",
    "    return np.array(vectors) / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsR-G4WYsu70"
   },
   "outputs": [],
   "source": [
    "sent_vec1 = []\n",
    "for each in dataset_p_l_rms_1:\n",
    "    temp = sentence_vectorizer(google_model, each)\n",
    "    if temp.shape[0] != 0:\n",
    "        sent_vec1.append(temp)\n",
    "    else:\n",
    "        sent_vec1.append(np.zeros((300,)))\n",
    "sent_vec1 = np.asarray(sent_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EsdRpmHGhpM"
   },
   "outputs": [],
   "source": [
    "sent_vec2 = []\n",
    "for each in dataset_p_l_rms_2:\n",
    "    temp = sentence_vectorizer(google_model, each)\n",
    "    if temp.shape[0] != 0:\n",
    "        sent_vec2.append(temp)\n",
    "    else:\n",
    "        sent_vec2.append(np.zeros((300,)))\n",
    "sent_vec2 = np.asarray(sent_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STeo9-I-He6R"
   },
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(np.asarray([sent_vec1[i]]), np.asarray([sent_vec2[i]]))[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(np.asarray([sent_vec1[i]]), np.asarray([sent_vec2[i]]))[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(np.asarray([sent_vec1[i]]), np.asarray([sent_vec2[i]]))[0][0]\n",
    "    distance_and_similarity_scores_4.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_4[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sent2Vec model(Fast.ai)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/epfml/sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Encoder V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join(\n",
    "            (str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "        corr,\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cmap=\"YlOrRd\")\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Textual Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):\n",
    "    message_embeddings_ = session_.run(\n",
    "        encoding_tensor, feed_dict={input_tensor_: messages_})\n",
    "    plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"I am an Indian\",\n",
    "    \"I am from India\",\n",
    "    \"I am not from India\",\n",
    "    \"I play cricket\",\n",
    "    \"I watch television\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    run_and_plot(session, similarity_input_placeholder, messages,\n",
    "         similarity_message_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elQTFbMuHhgN"
   },
   "source": [
    "**Siamese Neural Networks(Using LSTM and GRU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2639,
     "status": "ok",
     "timestamp": 1549095603688,
     "user": {
      "displayName": "Sri Vatsan",
      "photoUrl": "",
      "userId": "12441326585716590406"
     },
     "user_tz": -330
    },
    "id": "dE_9Xb7uHqgw",
    "outputId": "84a6f14b-0c50-4bb8-d8f5-b431d981f11b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as backend\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda, GRU, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2n_left = []\n",
    "for sentence in dataset_p_l_rms_l_1.tolist():\n",
    "    temp_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(inverse_vocabulary)\n",
    "            temp_sentence.append(len(inverse_vocabulary))\n",
    "            inverse_vocabulary.append(word)\n",
    "        else:\n",
    "            temp_sentence.append(vocabulary[word])\n",
    "    q2n_left.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2n_right = []\n",
    "for sentence in dataset_p_l_rms_l_2.tolist():\n",
    "    temp_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(inverse_vocabulary)\n",
    "            temp_sentence.append(len(inverse_vocabulary))\n",
    "            inverse_vocabulary.append(word)\n",
    "        else:\n",
    "            temp_sentence.append(vocabulary[word])\n",
    "    q2n_right.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = np.zeros((len(vocabulary) + 1, embedding_dim))\n",
    "embeddings[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in vocabulary.items():\n",
    "    if word in google_model.vocab:\n",
    "        embeddings[index] = google_model.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del google_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_left = q2n_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_right = q2n_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 0\n",
    "for each in range(length):\n",
    "    max_seq_length = max(max_seq_length, len(q2n_left[each]), len(q2n_right[each]))\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_left = pad_sequences(q2n_left, maxlen=max_seq_length)\n",
    "dataset_right = pad_sequences(q2n_right, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_left.shape == dataset_right.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 512\n",
    "n_hidden2 = 384\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WljRtYIvJcTn"
   },
   "outputs": [],
   "source": [
    "left_input = Input(shape=(max_seq_length, ), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length, ), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksWe8MukKbxW"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], \n",
    "                            input_length=max_seq_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0jTBGv4Kck8"
   },
   "outputs": [],
   "source": [
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5e9B1ChNKgA3"
   },
   "outputs": [],
   "source": [
    "shared_lstm1 = LSTM(n_hidden1, return_sequences=True)\n",
    "shared_dropout1 = Dropout(0.3)\n",
    "shared_gru1 = GRU(n_hidden2, return_sequences=True)\n",
    "shared_dropout2 = Dropout(0.4)\n",
    "shared_gru2 = GRU(n_hidden3, return_sequences=True)\n",
    "shared_dropout3 = Dropout(0.3)\n",
    "shared_lstm2 = LSTM(n_hidden4, return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiSpiz9uLGu3"
   },
   "outputs": [],
   "source": [
    "left_lstm1 = shared_lstm1(encoded_left)\n",
    "left_dropout1 = shared_dropout1(left_lstm1)\n",
    "left_gru1 = shared_gru1(left_dropout1)\n",
    "left_dropout2 = shared_dropout2(left_gru1)\n",
    "left_gru2 = shared_gru2(left_dropout2)\n",
    "left_dropout3 = shared_dropout3(left_gru2)\n",
    "left_lstm2 = shared_lstm2(left_dropout3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcQJfYn6Ma5S"
   },
   "outputs": [],
   "source": [
    "right_lstm1 = shared_lstm1(encoded_right)\n",
    "right_dropout1 = shared_dropout1(right_lstm1)\n",
    "right_gru1 = shared_gru1(right_dropout1)\n",
    "right_dropout2 = shared_dropout2(right_gru1)\n",
    "right_gru2 = shared_gru2(right_dropout2)\n",
    "right_dropout3 = shared_dropout3(right_gru2)\n",
    "right_lstm2 = shared_lstm2(right_dropout3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ug1R5gTGM4oR"
   },
   "outputs": [],
   "source": [
    "manhattan_distance_for_lstm = Lambda(function=lambda x: backend.exp(-backend.sum(backend.abs(x[0]-x[1]), axis=1, keepdims=True)),\n",
    "                                     output_shape=lambda x: (x[0][0], 1))([left_lstm2, right_lstm2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dWiDq3dUK7m"
   },
   "source": [
    "**Training and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fkjFhAkT8Ov"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNlq-mXyUjO9"
   },
   "outputs": [],
   "source": [
    "stratkfold = StratifiedKFold(n_splits=2, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5TGTK2CUrN8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 202174 samples, validate on 202174 samples\n",
      "Epoch 1/1\n",
      "  2048/202174 [..............................] - ETA: 9:04:12 - loss: 0.2159 - acc: 0.6372"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-689fc4cf0264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msiamese_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     siamese_network.fit([dataset_left[train_index], dataset_right[train_index]], is_duplicate_questions[train_index], batch_size=128, \n\u001b[1;32m----> 5\u001b[1;33m                         epochs=1, validation_data=([dataset_left[test_index], dataset_right[test_index]], is_duplicate_questions[test_index]))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_index, test_index in stratkfold.split(dataset_left, is_duplicate_questions):\n",
    "    siamese_network = Model([left_input, right_input], manhattan_distance_for_lstm)\n",
    "    siamese_network.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    siamese_network.fit([dataset_left[train_index], dataset_right[train_index]], is_duplicate_questions[train_index], batch_size=128, \n",
    "                        epochs=1, validation_data=([dataset_left[test_index], dataset_right[test_index]], is_duplicate_questions[test_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGyxnRuCHe6T"
   },
   "source": [
    "# Spell Corrector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8U9QUzGUHe6U"
   },
   "source": [
    "**Word Corrector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(words(open('data/big.txt').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(word, n=sum(WORDS.values())): \n",
    "    return WORDS[word] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word): \n",
    "    return max(candidates(word), key=probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word): \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    return set(w for w in words if w in WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(word):\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Em0idSRBHe6V"
   },
   "source": [
    "**Sentence Corrector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/words_dictionary.json\") as words_dictionary_file:\n",
    "    word_dict = json.load(words_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentences(sentence):\n",
    "    sentence = sentence.lower().split()\n",
    "    combination_sentences = []\n",
    "    combination_probabilities = []\n",
    "    meta_data = {}\n",
    "    for each in sentence:\n",
    "        if not word_dict.get(each, None):\n",
    "            possible_words = candidates(each)\n",
    "            probabilities = []\n",
    "            for each_word in possible_words:\n",
    "                probabilities.append(probability(each_word))\n",
    "            meta_data[each] = [list(possible_words), list(probabilities)]\n",
    "    for i in range(len(sentence)):\n",
    "        if meta_data.get(sentence[i], None):\n",
    "            for each in meta_data[sentence[i]][0]:\n",
    "                combination_sentences.append(\" \".join(sentence[:i]) + \" \" + each + \" \".join(sentence[i+1:]))\n",
    "    return combination_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every thing comes with a price', 'every thing comes with a prices']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sentences(\"Every thing comes with a pricee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Semantics_Similarity.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
