{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3hUGck-ksJp"
   },
   "source": [
    "# Semantics Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPXZlsook9cd"
   },
   "source": [
    "**Installing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqGfhdAakih2"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q numpy\n",
    "!pip install -U -q keras\n",
    "!pip install -U -q scikit-learn\n",
    "!pip install -U -q matplotlib\n",
    "!pip install -U -q nltk\n",
    "!pip install -U -q PyDrive \n",
    "!pip install -U -q pandas\n",
    "!pip install -U -q https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-win_amd64.whl\n",
    "!pip install -U -q torchvision\n",
    "!pip install --quiet tensorflow-hub\n",
    "!pip install --quiet seaborn\n",
    "!pip install --quiet \"tensorflow>=1.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "016VNjCml2NW"
   },
   "source": [
    "**Getting data from Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TEWzFSplqVP"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSqhf7dHl_SS"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqJZtt-HmF6v"
   },
   "outputs": [],
   "source": [
    "file_ids = [\"16-aKOfyeLQpBJlUHCJUGxWp4UsY2rvb3\", \"1oec77bHzg5a2oGshDuBe99jxMi80NlUo\"]\n",
    "file_names = [\"train_translated.csv\", \"test_translated.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLtygnFNmLx_"
   },
   "outputs": [],
   "source": [
    "for each_id, each_name in zip(file_ids, file_names):\n",
    "    download = drive.CreateFile({'id':each_id})\n",
    "    download.GetContentFile(each_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNPMenzA0Yyj"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3Z7qyzq0g6Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\"E:\\Datasets\\quora\\questions.csv\")\n",
    "questions.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions1 = questions.iloc[:, 3].values\n",
    "questions2 = questions.iloc[:, 4].values\n",
    "is_duplicate_questions = questions.iloc[:, 5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjk1awi9vns5"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNVsrA9WHe5b"
   },
   "outputs": [],
   "source": [
    "length = is_duplicate_questions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7q4ZLpcKj0v"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VsMTUtGdKoJM"
   },
   "source": [
    "**Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TEybJIOKkgT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "uRnXzsycKsMg",
    "outputId": "8b41cc96-5296-44c5-ca5a-9d62266da722",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vsriv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vsriv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vsriv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTPkxZdiK0BD"
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "stopword = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd75AVttK7hD"
   },
   "outputs": [],
   "source": [
    "sentences_1 = []\n",
    "for i in questions1:\n",
    "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
    "    tempx = tempx.lower().split()\n",
    "    tempx = [word for word in tempx if word not in stopword]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
    "    sentences_1.append(\" \".join(tempx))\n",
    "sentences_1 = np.asarray(sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdAWSHcJHe5q"
   },
   "outputs": [],
   "source": [
    "sentences_2 = []\n",
    "for i in questions2:\n",
    "    tempx = re.sub(r\"[^A-Za-z]\", \" \", str(i))\n",
    "    tempx = tempx.lower().split()\n",
    "    tempx = [word for word in tempx if word not in stopword]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"a\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"r\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"n\") for word in tempx]\n",
    "    tempx = [lemma.lemmatize(word, pos=\"v\") for word in tempx]\n",
    "    sentences_2.append(\" \".join(tempx))\n",
    "sentences_2 = np.asarray(sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astrology capricorn sun cap moon cap rise say\n",
      "triple capricorn sun moon ascendant capricorn say\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset_no = 5\n",
    "print(sentences_1[dataset_no])\n",
    "print(sentences_2[dataset_no])\n",
    "print(is_duplicate_questions[dataset_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count Vectorizor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.fit(np.append(sentences_1, sentences_2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_1 = count_vectorizer.transform(sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_2 = count_vectorizer.transform(sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_and_similarity_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(count_vectorizer_1[i], count_vectorizer_2[i])[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(count_vectorizer_1[i], count_vectorizer_2[i])[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(count_vectorizer_1[i], count_vectorizer_2[i])[0][0]\n",
    "    distance_and_similarity_scores.append(temp)\n",
    "#try to give this to log reg to find the similarity between these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tfidf Vectorizor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.fit(np.append(sentences_1, sentences_2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vectorizer_1 = tfidf_vectorizer.transform(sentences_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vectorizer_2 = tfidf_vectorizer.transform(sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(tfid_vectorizer_1[i], tfid_vectorizer_2[i])[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(tfid_vectorizer_1[i], tfid_vectorizer_2[i])[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(tfid_vectorizer_1[i], tfid_vectorizer_2[i])[0][0]\n",
    "    distance_and_similarity_scores_2.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-xcGFtPGdfJ"
   },
   "source": [
    "**LSA Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYXzCvrQGgm3"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHMV-1Bhai1e"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2DObvs6mal97"
   },
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=300,\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__L-nT-lapFP"
   },
   "outputs": [],
   "source": [
    "lsa_model1 = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCFrhTD-He55"
   },
   "outputs": [],
   "source": [
    "lsa_model2 = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0Qm7cU-asMM"
   },
   "outputs": [],
   "source": [
    "lsa_1 = lsa_model1.fit_transform(sentences_1)\n",
    "lsa_2 = lsa_model2.fit_transform(sentences_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShsDeBj0He59"
   },
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-RqCVPvHe5_"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(np.asarray([lsa_1[i]]), np.asarray([lsa_2[i]]))[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(np.asarray([lsa_1[i]]), np.asarray([lsa_2[i]]))[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(np.asarray([lsa_1[i]]), np.asarray([lsa_2[i]]))[0][0]\n",
    "    distance_and_similarity_scores_3.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHVZTTRYHe6A",
    "outputId": "cc91ef3d-720c-4269-f4f8-c6729caa3889"
   },
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_3[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uENvSaywLgrP"
   },
   "source": [
    "**Word2Vec model(Using Mean to get the sentence vectors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_vSYQP8LN36"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61vPjXTNB8Ur"
   },
   "outputs": [],
   "source": [
    "google_model = KeyedVectors.load_word2vec_format(\"E:\\Models\\pre_trained\\word2vec\\google\\google.300d.bin\", binary=True)\n",
    "#wiki_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/wiki/wiki.300d.txt\", binary=False)\n",
    "#common_crawl_model = KeyedVectors.load_word2vec_format(\"models/pretrained/glove/common_crawl/common_crawl.300d.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOUcqYwzLzTf"
   },
   "outputs": [],
   "source": [
    "def sentence_vectorizer(model, sentence):\n",
    "    vectors =[]\n",
    "    num = 0\n",
    "    for i in sentence.split():\n",
    "        try:\n",
    "            if num == 0:\n",
    "                vectors = model[i]\n",
    "            else:\n",
    "                vectors = np.add(vectors, model[i])\n",
    "            num += 1\n",
    "        except:\n",
    "            pass\n",
    "    return np.array(vectors) / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsR-G4WYsu70"
   },
   "outputs": [],
   "source": [
    "sent_vec1 = []\n",
    "for each in sentences_1:\n",
    "    temp = sentence_vectorizer(google_model, each)\n",
    "    if temp.shape[0] != 0:\n",
    "        sent_vec1.append(temp)\n",
    "    else:\n",
    "        sent_vec1.append(np.zeros((300,)))\n",
    "sent_vec1 = np.asarray(sent_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EsdRpmHGhpM"
   },
   "outputs": [],
   "source": [
    "sent_vec2 = []\n",
    "for each in sentences_2:\n",
    "    temp = sentence_vectorizer(google_model, each)\n",
    "    if temp.shape[0] != 0:\n",
    "        sent_vec2.append(temp)\n",
    "    else:\n",
    "        sent_vec2.append(np.zeros((300,)))\n",
    "sent_vec2 = np.asarray(sent_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STeo9-I-He6R"
   },
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(np.asarray([sent_vec1[i]]), np.asarray([sent_vec2[i]]))[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(np.asarray([sent_vec1[i]]), np.asarray([sent_vec2[i]]))[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(np.asarray([sent_vec1[i]]), np.asarray([sent_vec2[i]]))[0][0]\n",
    "    distance_and_similarity_scores_4.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_4[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abisek Update this and use quora questions dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**InferText model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.infersent.models import InferSent\n",
    "model_version = 1\n",
    "MODEL_PATH = \"models\\infersent\\infersent%s.pickle\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "model = model.cuda() if use_cuda else model # Keep it on CPU or put it on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = 'E:\\Models\\pre_trained\\glove\\commoncrawl\\common_crawl.300d.txt'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4096)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 139/152 (91.4%)\n",
      "Speed : 25.0 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(sentences_1[:20], bsize=128, tokenize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 140/150 (93.3%)\n",
      "Speed : 35.7 sentences/s (cpu mode, bsize=128)\n"
     ]
    }
   ],
   "source": [
    "embeddings_2 = model.encode(sentences_2[:20], bsize=128, tokenize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_6 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(np.asarray([embeddings_1[i]]), np.asarray([embeddings_2[i]]))[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(np.asarray([embeddings_1[i]]), np.asarray([embeddings_2[i]]))[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(np.asarray([embeddings_1[i]]), np.asarray([embeddings_2[i]]))[0][0]\n",
    "    distance_and_similarity_scores_6.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'cosine_similarity': 0.9458276, 'manhattan_distance': 1.2538241, 'euclidean_distance': 32.184387501109086}\n",
      "0 {'cosine_similarity': 0.6067528, 'manhattan_distance': 2.9255688, 'euclidean_distance': 112.79561321428446}\n",
      "0 {'cosine_similarity': 0.8873254, 'manhattan_distance': 1.8598092, 'euclidean_distance': 72.58798747658739}\n",
      "0 {'cosine_similarity': 0.5393939, 'manhattan_distance': 3.1896555, 'euclidean_distance': 120.62491884012707}\n",
      "0 {'cosine_similarity': 0.75635266, 'manhattan_distance': 3.1802003, 'euclidean_distance': 131.53287864351518}\n",
      "1 {'cosine_similarity': 0.8199016, 'manhattan_distance': 2.2952108, 'euclidean_distance': 86.52014614462496}\n",
      "0 {'cosine_similarity': 0.3553719, 'manhattan_distance': 3.3347626, 'euclidean_distance': 130.02840969695353}\n",
      "1 {'cosine_similarity': 0.96099097, 'manhattan_distance': 0.75167567, 'euclidean_distance': 19.646674617991266}\n",
      "0 {'cosine_similarity': 1.0, 'manhattan_distance': 0.0, 'euclidean_distance': 1.2594475265359506e-05}\n",
      "0 {'cosine_similarity': 0.7971726, 'manhattan_distance': 2.2010882, 'euclidean_distance': 83.40903483864076}\n",
      "0 {'cosine_similarity': 0.59591144, 'manhattan_distance': 3.379172, 'euclidean_distance': 141.06986477379178}\n",
      "1 {'cosine_similarity': 0.9302225, 'manhattan_distance': 1.104042, 'euclidean_distance': 31.58952859152646}\n",
      "1 {'cosine_similarity': 1.0000002, 'manhattan_distance': 0.0, 'euclidean_distance': 4.345100023783743e-06}\n",
      "1 {'cosine_similarity': 0.960274, 'manhattan_distance': 0.72040766, 'euclidean_distance': 17.49958434590917}\n",
      "0 {'cosine_similarity': 0.970024, 'manhattan_distance': 1.1597912, 'euclidean_distance': 27.432081533150267}\n",
      "1 {'cosine_similarity': 0.8476085, 'manhattan_distance': 2.145061, 'euclidean_distance': 87.41615762085985}\n",
      "1 {'cosine_similarity': 1.0000001, 'manhattan_distance': 0.0, 'euclidean_distance': 0.0}\n",
      "0 {'cosine_similarity': 0.8766564, 'manhattan_distance': 1.6476896, 'euclidean_distance': 58.87347629003125}\n",
      "1 {'cosine_similarity': 0.86601734, 'manhattan_distance': 1.7946079, 'euclidean_distance': 64.51746689830156}\n",
      "0 {'cosine_similarity': 0.8747243, 'manhattan_distance': 1.8397535, 'euclidean_distance': 60.018542632348726}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_6[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence Encoder V2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tfhub.dev/google/universal-sentence-encoder/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --quiet tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0310 12:42:12.050889  7768 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0310 13:10:26.869572  7768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    sentences_embeddings_1 = session.run(embed(sentences_1[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0310 13:10:56.016236  7768 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    sentences_embeddings_2 = session.run(embed(sentences_2[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_and_similarity_scores_8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    temp = {}\n",
    "    temp['cosine_similarity'] = cosine_similarity(np.asarray([sentences_embeddings_1[i]]), np.asarray([sentences_embeddings_2[i]]))[0][0]\n",
    "    temp['manhattan_distance'] = euclidean_distances(np.asarray([sentences_embeddings_1[i]]), np.asarray([sentences_embeddings_2[i]]))[0][0]\n",
    "    temp['euclidean_distance'] = manhattan_distances(np.asarray([sentences_embeddings_1[i]]), np.asarray([sentences_embeddings_2[i]]))[0][0]\n",
    "    distance_and_similarity_scores_8.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'cosine_similarity': 0.9486568, 'manhattan_distance': 0.3204472, 'euclidean_distance': 5.245485807812656}\n",
      "0 {'cosine_similarity': 0.6202224, 'manhattan_distance': 0.8715247, 'euclidean_distance': 14.994461547388255}\n",
      "0 {'cosine_similarity': 0.8335015, 'manhattan_distance': 0.57705903, 'euclidean_distance': 9.815707650494005}\n",
      "0 {'cosine_similarity': 0.2900136, 'manhattan_distance': 1.1916263, 'euclidean_distance': 21.74811126565328}\n",
      "0 {'cosine_similarity': 0.49464017, 'manhattan_distance': 1.0053457, 'euclidean_distance': 17.698465278358526}\n",
      "1 {'cosine_similarity': 0.8778694, 'manhattan_distance': 0.49422783, 'euclidean_distance': 7.790173144967412}\n",
      "0 {'cosine_similarity': 0.16489124, 'manhattan_distance': 1.2923691, 'euclidean_distance': 23.196336368258926}\n",
      "1 {'cosine_similarity': 0.9448141, 'manhattan_distance': 0.33222255, 'euclidean_distance': 5.783593213651329}\n",
      "0 {'cosine_similarity': 1.0, 'manhattan_distance': 0.0, 'euclidean_distance': 0.0}\n",
      "0 {'cosine_similarity': 0.83476865, 'manhattan_distance': 0.574859, 'euclidean_distance': 9.804769292395576}\n",
      "0 {'cosine_similarity': 0.12745847, 'manhattan_distance': 1.3210161, 'euclidean_distance': 23.505938380429143}\n",
      "1 {'cosine_similarity': 0.9092075, 'manhattan_distance': 0.42612785, 'euclidean_distance': 7.400624245405197}\n",
      "1 {'cosine_similarity': 1.0, 'manhattan_distance': 0.0, 'euclidean_distance': 0.0}\n",
      "1 {'cosine_similarity': 0.95513123, 'manhattan_distance': 0.29956242, 'euclidean_distance': 5.218573479010956}\n",
      "0 {'cosine_similarity': 0.97338766, 'manhattan_distance': 0.23070462, 'euclidean_distance': 3.2130904488731176}\n",
      "1 {'cosine_similarity': 0.68606377, 'manhattan_distance': 0.7923839, 'euclidean_distance': 13.379268934691936}\n",
      "1 {'cosine_similarity': 1.0000001, 'manhattan_distance': 0.0, 'euclidean_distance': 0.0}\n",
      "0 {'cosine_similarity': 0.810645, 'manhattan_distance': 0.6153944, 'euclidean_distance': 10.599712683309917}\n",
      "1 {'cosine_similarity': 0.8176981, 'manhattan_distance': 0.60382426, 'euclidean_distance': 10.594791327503117}\n",
      "0 {'cosine_similarity': 0.928566, 'manhattan_distance': 0.37797904, 'euclidean_distance': 6.199734789945069}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 20):\n",
    "    print(is_duplicate_questions[i], distance_and_similarity_scores_8[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN and TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/zhihang/an-ensemble-approach-cnn-and-timedistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, time, json\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = dataset_p_l_rms_l_1.tolist() + dataset_p_l_rms_l_2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_word_sequences = tokenizer.texts_to_sequences(sentences_1.tolist())\n",
    "question2_word_sequences = tokenizer.texts_to_sequences(sentences_2.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "max_question_len = 0\n",
    "for each in range(length):\n",
    "    max_question_len = max(max_question_len, len(question1_word_sequences[each]), len(question2_word_sequences[each]))\n",
    "print(max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1 = pad_sequences(question1_word_sequences,\n",
    "                              maxlen = max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q2 = pad_sequences(question2_word_sequences,\n",
    "                              maxlen = max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 400001\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('E:/Models/pre_trained/glove/wiki/wiki.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 20431\n"
     ]
    }
   ],
   "source": [
    "nb_words = len(word_index)\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0)) #75,334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 128 # Number of nodes in the Dense layers\n",
    "dropout = 0.25 # Percentage of nodes to drop\n",
    "nb_filter = 32 # Number of filters to use in Convolution1D\n",
    "filter_length = 3 # Length of filter for Convolution1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\n",
    "bias = bias_initializer='zeros'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_1_input')\n",
    "model_1_embedding = Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     weights = [word_embedding_matrix], \n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False)(model_1_input)\n",
    "model_1_conv_a = Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same')(model_1_embedding)\n",
    "model_1_batch_a = BatchNormalization()(model_1_conv_a)\n",
    "model_1_act = Activation('relu')(model_1_batch_a)\n",
    "model_1_drop_a = Dropout(dropout)(model_1_act)\n",
    "model_1_conv_b = Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same')(model_1_drop_a)\n",
    "model_1_batch_b = BatchNormalization()(model_1_conv_b)\n",
    "model_1_act_b = Activation('relu')(model_1_batch_b)\n",
    "model_1_drop_b = Dropout(dropout)(model_1_act_b)\n",
    "model_1_flat = Flatten()(model_1_drop_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_2_input')\n",
    "model_2_embedding = Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     weights = [word_embedding_matrix], \n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False)(model_2_input)\n",
    "model_2_conv_a = Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same')(model_2_embedding)\n",
    "model_2_batch_a = BatchNormalization()(model_2_conv_a)\n",
    "model_2_act = Activation('relu')(model_2_batch_a)\n",
    "model_2_drop_a = Dropout(dropout)(model_2_act)\n",
    "model_2_conv_b = Convolution1D(filters = nb_filter, \n",
    "                         kernel_size = filter_length, \n",
    "                         padding = 'same')(model_2_drop_a)\n",
    "model_2_batch_b = BatchNormalization()(model_2_conv_b)\n",
    "model_2_act_b = Activation('relu')(model_2_batch_b)\n",
    "model_2_drop_b = Dropout(dropout)(model_2_act_b)\n",
    "model_2_flat = Flatten()(model_2_drop_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_3_input')\n",
    "model_3_embedding = Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     weights = [word_embedding_matrix],\n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False)(model_3_input)\n",
    "model_3_time_distributed = TimeDistributed(Dense(embedding_dim))(model_3_embedding)\n",
    "model_3_batch = BatchNormalization()(model_3_time_distributed)\n",
    "model_3_act = Activation('relu')(model_3_batch)\n",
    "model_3_drop = Dropout(dropout)(model_3_act)\n",
    "model_3_lambda = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, ))(model_3_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_input = Input(shape = (max_question_len,), dtype = 'int32', name = 'model_4_input')\n",
    "model_4_embedding = Embedding(nb_words + 1,\n",
    "                     embedding_dim,\n",
    "                     weights = [word_embedding_matrix],\n",
    "                     input_length = max_question_len,\n",
    "                     trainable = False)(model_4_input)\n",
    "model_4_time_distributed = TimeDistributed(Dense(embedding_dim))(model_4_embedding)\n",
    "model_4_batch = BatchNormalization()(model_4_time_distributed)\n",
    "model_4_act = Activation('relu')(model_4_batch)\n",
    "model_4_drop = Dropout(dropout)(model_4_act)\n",
    "model_4_lambda = Lambda(lambda x: K.max(x, axis=1), output_shape=(embedding_dim, ))(model_4_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_layer = concatenate([model_1_flat, model_2_flat, model_3_lambda, model_4_lambda], name = 'merge_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dense(200, activation = 'relu', name = 'dense1')(merge_layer)\n",
    "t = Dropout(0.3)(t)\n",
    "t = BatchNormalization()(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dense(200, activation = 'relu', name  ='dense2')(t)\n",
    "t = Dropout(0.3)(t)\n",
    "t = BatchNormalization()(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dense(100, activation= 'relu',name = 'dense3')(t)\n",
    "t = Dropout(0.3)(t)\n",
    "t = BatchNormalization()(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = Dense(1, activation = 'sigmoid')(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = [model_1_input, model_2_input, model_3_input, model_4_input], outputs = final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_weights = 'question_pairs_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 343695 samples, validate on 60653 samples\n",
      "Epoch 1/1\n",
      "  7680/343695 [..............................] - ETA: 58:10 - loss: 0.7693 - acc: 0.5598"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-5d6ff2285de8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                     callbacks=callbacks)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Minutes elapsed: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vsriv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(save_best_weights, monitor='val_loss', save_best_only=True),\n",
    "             EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')]\n",
    "history = model.fit([train_q1, train_q2, train_q1, train_q2],\n",
    "                    questions.is_duplicate,\n",
    "                    batch_size=256,\n",
    "                    epochs=1, #Use 100, I reduce it for Kaggle,\n",
    "                    validation_split=0.15,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                              'train_acc': history.history['acc'],\n",
    "                              'valid_acc': history.history['val_acc'],\n",
    "                              'train_loss': history.history['loss'],\n",
    "                              'valid_loss': history.history['val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(summary_stats.train_loss) # blue\n",
    "plt.plot(summary_stats.valid_loss) # green\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss, idx = min((loss, idx) for (idx, loss) in enumerate(history.history['val_loss']))\n",
    "print('Minimum loss at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(min_loss))\n",
    "min_loss = round(min_loss, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(save_best_weights)\n",
    "predictions = model.predict([test_q1, test_q2, test_q1, test_q2], verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elQTFbMuHhgN"
   },
   "source": [
    "**Siamese Neural Networks(Using LSTM and GRU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2639,
     "status": "ok",
     "timestamp": 1549095603688,
     "user": {
      "displayName": "Sri Vatsan",
      "photoUrl": "",
      "userId": "12441326585716590406"
     },
     "user_tz": -330
    },
    "id": "dE_9Xb7uHqgw",
    "outputId": "84a6f14b-0c50-4bb8-d8f5-b431d981f11b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as backend\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda, GRU, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2n_left = []\n",
    "for sentence in sentences_1.tolist():\n",
    "    temp_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(inverse_vocabulary)\n",
    "            temp_sentence.append(len(inverse_vocabulary))\n",
    "            inverse_vocabulary.append(word)\n",
    "        else:\n",
    "            temp_sentence.append(vocabulary[word])\n",
    "    q2n_left.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2n_right = []\n",
    "for sentence in sentences_2.tolist():\n",
    "    temp_sentence = []\n",
    "    for word in sentence.split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(inverse_vocabulary)\n",
    "            temp_sentence.append(len(inverse_vocabulary))\n",
    "            inverse_vocabulary.append(word)\n",
    "        else:\n",
    "            temp_sentence.append(vocabulary[word])\n",
    "    q2n_right.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embeddings = np.zeros((len(vocabulary) + 1, embedding_dim))\n",
    "embeddings[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in vocabulary.items():\n",
    "    if word in google_model.vocab:\n",
    "        embeddings[index] = google_model.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del google_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_left = q2n_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_right = q2n_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 0\n",
    "for each in range(length):\n",
    "    max_seq_length = max(max_seq_length, len(q2n_left[each]), len(q2n_right[each]))\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_left = pad_sequences(q2n_left, maxlen=max_seq_length)\n",
    "dataset_right = pad_sequences(q2n_right, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_left.shape == dataset_right.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 512\n",
    "n_hidden2 = 384\n",
    "n_hidden3 = 256\n",
    "n_hidden4 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WljRtYIvJcTn"
   },
   "outputs": [],
   "source": [
    "left_input = Input(shape=(max_seq_length, ), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length, ), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksWe8MukKbxW"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], \n",
    "                            input_length=max_seq_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0jTBGv4Kck8"
   },
   "outputs": [],
   "source": [
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5e9B1ChNKgA3"
   },
   "outputs": [],
   "source": [
    "shared_lstm1 = LSTM(n_hidden1, return_sequences=True)\n",
    "shared_dropout1 = Dropout(0.3)\n",
    "shared_gru1 = GRU(n_hidden2, return_sequences=True)\n",
    "shared_dropout2 = Dropout(0.4)\n",
    "shared_gru2 = GRU(n_hidden3, return_sequences=True)\n",
    "shared_dropout3 = Dropout(0.3)\n",
    "shared_lstm2 = LSTM(n_hidden4, return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiSpiz9uLGu3"
   },
   "outputs": [],
   "source": [
    "left_lstm1 = shared_lstm1(encoded_left)\n",
    "left_dropout1 = shared_dropout1(left_lstm1)\n",
    "left_gru1 = shared_gru1(left_dropout1)\n",
    "left_dropout2 = shared_dropout2(left_gru1)\n",
    "left_gru2 = shared_gru2(left_dropout2)\n",
    "left_dropout3 = shared_dropout3(left_gru2)\n",
    "left_lstm2 = shared_lstm2(left_dropout3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcQJfYn6Ma5S"
   },
   "outputs": [],
   "source": [
    "right_lstm1 = shared_lstm1(encoded_right)\n",
    "right_dropout1 = shared_dropout1(right_lstm1)\n",
    "right_gru1 = shared_gru1(right_dropout1)\n",
    "right_dropout2 = shared_dropout2(right_gru1)\n",
    "right_gru2 = shared_gru2(right_dropout2)\n",
    "right_dropout3 = shared_dropout3(right_gru2)\n",
    "right_lstm2 = shared_lstm2(right_dropout3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ug1R5gTGM4oR"
   },
   "outputs": [],
   "source": [
    "manhattan_distance_for_lstm = Lambda(function=lambda x: backend.exp(-backend.sum(backend.abs(x[0]-x[1]), axis=1, keepdims=True)),\n",
    "                                     output_shape=lambda x: (x[0][0], 1))([left_lstm2, right_lstm2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dWiDq3dUK7m"
   },
   "source": [
    "**Training and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fkjFhAkT8Ov"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNlq-mXyUjO9"
   },
   "outputs": [],
   "source": [
    "stratkfold = StratifiedKFold(n_splits=2, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n5TGTK2CUrN8"
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in stratkfold.split(dataset_left, is_duplicate_questions):\n",
    "    siamese_network = Model([left_input, right_input], manhattan_distance_for_lstm)\n",
    "    siamese_network.compile(loss='mean_squared_error', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    siamese_network.fit([dataset_left[train_index], dataset_right[train_index]], is_duplicate_questions[train_index], batch_size=128, \n",
    "                        epochs=128, validation_data=([dataset_left[test_index], dataset_right[test_index]], is_duplicate_questions[test_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGyxnRuCHe6T"
   },
   "source": [
    "# Spell Corrector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8U9QUzGUHe6U"
   },
   "source": [
    "**Word Corrector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(words(open('data/big.txt').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(word, n=sum(WORDS.values())): \n",
    "    return WORDS[word] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word): \n",
    "    return max(candidates(word), key=probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word): \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    return set(w for w in words if w in WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(word):\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Em0idSRBHe6V"
   },
   "source": [
    "**Sentence Corrector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/words_dictionary.json\") as words_dictionary_file:\n",
    "    word_dict = json.load(words_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sentences(sentence):\n",
    "    sentence = sentence.lower().split()\n",
    "    combination_sentences = []\n",
    "    combination_probabilities = []\n",
    "    meta_data = {}\n",
    "    for each in sentence:\n",
    "        if not word_dict.get(each, None):\n",
    "            possible_words = candidates(each)\n",
    "            probabilities = []\n",
    "            for each_word in possible_words:\n",
    "                probabilities.append(probability(each_word))\n",
    "            meta_data[each] = [list(possible_words), list(probabilities)]\n",
    "    for i in range(len(sentence)):\n",
    "        if meta_data.get(sentence[i], None):\n",
    "            for each in meta_data[sentence[i]][0]:\n",
    "                combination_sentences.append(\" \".join(sentence[:i]) + \" \" + each + \" \".join(sentence[i+1:]))\n",
    "    return combination_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sentences(\"Every thing comes with a pricee\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Semantics_Similarity.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
